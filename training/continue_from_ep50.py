"""
Continue training from Episode 50 checkpoint to Episode 100
Loads checkpoint_ep50.pth and continues with updated hyperparameters
"""

import os
import sys
import numpy as np
from datetime import datetime
from tqdm import tqdm

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Add SUMO tools to path if SUMO_HOME is set
if "SUMO_HOME" in os.environ:
    tools = os.path.join(os.environ["SUMO_HOME"], "tools")
    if tools not in sys.path:
        sys.path.append(tools)

from controls.ml_based.drl.agent import DQNAgent
from controls.ml_based.drl.traffic_management import TrafficManagement
from controls.ml_based.drl.config import DRLConfig
from route_generator.traffic_config import get_traffic_config
from route_generator import generate_all_routes_developed
from training.train_drl import TrainingLogger


def continue_training():
    """
    Continue training from episode 50 to episode 100
    """
    print("=" * 70)
    print("CONTINUING TRAINING FROM EPISODE 50 TO 100")
    print("=" * 70)

    # Checkpoint path - using the actual trained model
    checkpoint_path = "models/training_20251016_200158/checkpoint_ep50.pth"

    if not os.path.exists(checkpoint_path):
        print(f"ERROR: Checkpoint not found at {checkpoint_path}")
        return

    print(f"\nLoading checkpoint: {checkpoint_path}")
    print("Current hyperparameters (after fixes):")
    print(f"  ALPHA_WAIT: {DRLConfig.ALPHA_WAIT}")
    print(f"  ALPHA_SYNC: {DRLConfig.ALPHA_SYNC}")
    print(f"  ALPHA_PED_DEMAND: {DRLConfig.ALPHA_PED_DEMAND}")
    print(f"  EPSILON_DECAY: {DRLConfig.EPSILON_DECAY}")
    print("  Target episodes: 51-100 (50 more episodes)\n")

    # Setup new log directory for continuation
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = f"logs/training_continue_{timestamp}"
    model_dir = f"models/training_continue_{timestamp}"
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(model_dir, exist_ok=True)

    # Initialize environment
    sumo_config = "test.sumocfg"
    tls_ids = ["3", "6"]
    env = TrafficManagement(sumo_config, tls_ids, gui=False)

    # Get state dimension
    initial_state = env.reset()
    state_dim = len(initial_state)
    action_dim = DRLConfig.ACTION_DIM
    env.close()

    print(f"State dimension: {state_dim}")
    print(f"Action dimension: {action_dim}")

    # Initialize agent
    agent = DQNAgent(state_dim, action_dim)

    # Load checkpoint
    agent.load(checkpoint_path)

    print("\nCheckpoint loaded successfully!")
    print(f"  Starting from episode: {agent.episode_count}")
    print(f"  Current epsilon: {agent.epsilon:.4f}")
    print(f"  Training steps completed: {agent.steps}")

    # Initialize logger
    logger = TrainingLogger(log_dir)

    # Training loop - episodes 51 to 100
    start_episode = agent.episode_count
    end_episode = 100
    num_episodes = end_episode - start_episode

    print(f"\nStarting training for episodes {start_episode + 1} to {end_episode}...")
    print(f"Logs will be saved to: {log_dir}")
    print(f"Models will be saved to: {model_dir}\n")

    # Note: Initial routes already generated by shell script

    for episode_idx in tqdm(range(num_episodes), desc="Training"):
        episode = start_episode + episode_idx + 1  # Actual episode number (51, 52, ...)

        # Generate new routes for each episode
        if episode_idx > 0:
            traffic_config = get_traffic_config()
            print(f"\n{'=' * 70}")
            print(f"Episode {episode} - Generating routes:")
            print(f"  Cars: {traffic_config['cars']}/hr")
            print(f"  Bicycles: {traffic_config['bicycles']}/hr")
            print(f"  Pedestrians: {traffic_config['pedestrians']}/hr")
            print(f"  Buses: {traffic_config['buses']}")
            print(f"{'=' * 70}")
            generate_all_routes_developed(traffic_config)
            # Close previous episode's SUMO connection
            env.close()

        # Reset environment (starts new SUMO instance)
        state = env.reset()
        env.reward_calculator.reset()

        episode_reward = 0
        episode_losses = []
        step_count = 0
        episode_metrics = {
            "avg_waiting_time": [],
            "waiting_time_car": [],
            "waiting_time_bicycle": [],
            "waiting_time_bus": [],
            "waiting_time_pedestrian": [],
            "sync_success_count": 0,
            "pedestrian_phase_count": 0,
            "reward_waiting": [],
            "reward_flow": [],
            "reward_sync": [],
            "reward_co2": [],
            "reward_equity": [],
            "reward_safety": [],
            "reward_pedestrian": [],
            "safety_violation_count": 0,
            "ped_demand_ignored_count": 0,
        }

        # Episode loop
        for step in range(DRLConfig.MAX_STEPS_PER_EPISODE):
            # Select action
            action = agent.select_action(state, explore=True)

            # Take step
            next_state, reward, done, info = env.step(action)

            # Store experience
            agent.store_experience(state, action, reward, next_state, done, info)

            # Train agent
            if step % DRLConfig.UPDATE_FREQUENCY == 0:
                loss = agent.train()
                if loss is not None:
                    episode_losses.append(loss)

            # Update state and metrics
            state = next_state
            episode_reward += reward
            step_count += 1

            # Track metrics
            episode_metrics["avg_waiting_time"].append(info.get("waiting_time", 0))
            episode_metrics["waiting_time_car"].append(info.get("waiting_time_car", 0))
            episode_metrics["waiting_time_bicycle"].append(
                info.get("waiting_time_bicycle", 0)
            )
            episode_metrics["waiting_time_bus"].append(info.get("waiting_time_bus", 0))
            episode_metrics["waiting_time_pedestrian"].append(
                info.get("waiting_time_pedestrian", 0)
            )
            if info.get("sync_achieved", False):
                episode_metrics["sync_success_count"] += 1
            if info.get("event_type") == "pedestrian_phase":
                episode_metrics["pedestrian_phase_count"] += 1

            # Track reward components (directly from info dict)
            episode_metrics["reward_waiting"].append(info.get("reward_waiting", 0))
            episode_metrics["reward_flow"].append(info.get("reward_flow", 0))
            episode_metrics["reward_sync"].append(info.get("reward_sync", 0))
            episode_metrics["reward_co2"].append(info.get("reward_co2", 0))
            episode_metrics["reward_equity"].append(info.get("reward_equity", 0))
            episode_metrics["reward_safety"].append(info.get("reward_safety", 0))
            episode_metrics["reward_pedestrian"].append(
                info.get("reward_pedestrian", 0)
            )

            # Track safety violations
            if info.get("safety_violation", False):
                episode_metrics["safety_violation_count"] += 1
            if info.get("ped_demand_ignored", False):
                episode_metrics["ped_demand_ignored_count"] += 1

            if done:
                break

        # Update epsilon
        agent.decay_epsilon()
        agent.episode_count = episode  # Update episode counter

        # Calculate episode statistics
        avg_loss = np.mean(episode_losses) if episode_losses else None
        avg_reward = episode_reward / step_count if step_count > 0 else 0
        final_metrics = {
            "avg_waiting_time": np.mean(episode_metrics["avg_waiting_time"]),
            "waiting_time_car": np.mean(episode_metrics["waiting_time_car"]),
            "waiting_time_bicycle": np.mean(episode_metrics["waiting_time_bicycle"]),
            "waiting_time_bus": np.mean(episode_metrics["waiting_time_bus"]),
            "waiting_time_pedestrian": np.mean(
                episode_metrics["waiting_time_pedestrian"]
            ),
            "sync_success_rate": (
                episode_metrics["sync_success_count"] / step_count
                if step_count > 0
                else 0
            ),
            "pedestrian_phase_count": episode_metrics["pedestrian_phase_count"],
            "reward_waiting_avg": np.mean(episode_metrics["reward_waiting"]),
            "reward_flow_avg": np.mean(episode_metrics["reward_flow"]),
            "reward_sync_avg": np.mean(episode_metrics["reward_sync"]),
            "reward_co2_avg": np.mean(episode_metrics["reward_co2"]),
            "reward_equity_avg": np.mean(episode_metrics["reward_equity"]),
            "reward_safety_avg": np.mean(episode_metrics["reward_safety"]),
            "reward_pedestrian_avg": np.mean(episode_metrics["reward_pedestrian"]),
            "safety_violation_count": episode_metrics["safety_violation_count"],
            "safety_violation_rate": (
                episode_metrics["safety_violation_count"] / step_count
                if step_count > 0
                else 0
            ),
            "ped_demand_ignored_count": episode_metrics["ped_demand_ignored_count"],
            "ped_demand_ignored_rate": (
                episode_metrics["ped_demand_ignored_count"] / step_count
                if step_count > 0
                else 0
            ),
        }

        # Log episode
        logger.log_episode(
            episode, avg_reward, avg_loss, step_count, agent.epsilon, final_metrics
        )

        # Print progress
        print(f"\nEpisode {episode} Complete:")
        loss_str = f"{avg_loss:.4f}" if avg_loss is not None else "N/A"
        print(
            f"  Reward: {avg_reward:.4f} | Loss: {loss_str} | Steps: {step_count} | Epsilon: {agent.epsilon:.3f}"
        )
        print(
            f"  Avg Wait: {final_metrics['avg_waiting_time']:.2f}s | Sync Rate: {final_metrics['sync_success_rate']:.2%}"
        )
        print(
            f"  Car: {final_metrics['waiting_time_car']:.2f}s | Bike: {final_metrics['waiting_time_bicycle']:.2f}s | Bus: {final_metrics['waiting_time_bus']:.2f}s"
        )

        # Save logs every episode
        logger.save_logs()

        # Save model checkpoints every 10 episodes
        if episode % 10 == 0:
            checkpoint_path = os.path.join(model_dir, f"checkpoint_ep{episode}.pth")
            agent.save(checkpoint_path)
            logger.plot_training_progress()

    # Save final model
    final_model_path = os.path.join(model_dir, "final_model_ep100.pth")
    agent.save(final_model_path)
    logger.save_logs()
    logger.plot_training_progress()
    env.close()

    print(f"\n{'=' * 70}")
    print("TRAINING COMPLETE!")
    print(f"{'=' * 70}")
    print(f"Final model saved to: {final_model_path}")
    print(f"Logs saved to: {log_dir}")
    print(f"Total episodes: 51-100 ({num_episodes} episodes)")
    print(f"Final epsilon: {agent.epsilon:.4f}\n")


if __name__ == "__main__":
    continue_training()
