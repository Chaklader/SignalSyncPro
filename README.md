# SignalSyncPro üö¶

**Advanced Traffic Signal Control using Deep Reinforcement Learning**

SignalSyncPro is a sophisticated traffic management system that uses Deep Q-Network (DQN) reinforcement learning to
optimize traffic signal timing at intersections. The system learns to minimize vehicle waiting times while maintaining
safe and efficient traffic flow across multiple vehicle types (cars, bicycles, buses, and pedestrians).

<img src="images/simulation.png" width="400" height="auto" alt="SUMO Traffic Simulation">

---

## üåü Features

### **Intelligent Traffic Control**

- **Deep Reinforcement Learning**: DQN-based agent that learns optimal signal timing policies
- **Multi-Modal Support**: Handles cars, bicycles, buses, and pedestrians
- **Adaptive Learning**: Trains on diverse traffic scenarios for robust performance
- **Real-Time Optimization**: Minimizes waiting times while ensuring safety

### **Comprehensive Testing**

- **30 Test Scenarios**: Systematic evaluation across varying traffic conditions
- **Baseline Comparison**: Rule-based "developed" control for performance benchmarking
- **Detailed Metrics**: Tracks waiting times, sync rates, safety violations, and emissions

### **Production-Ready Architecture**

- **Modular Design**: Clean separation of concerns (controls, routes, infrastructure)
- **Reusable Utilities**: Centralized path setup and traffic configuration
- **Flexible Configuration**: Easy-to-modify constants and parameters
- **Comprehensive Logging**: Detailed training and testing logs

---

## üìã Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Training](#training)
- [Testing](#testing)
- [Configuration](#configuration)
- [Results](#results)
- [Architecture](#architecture)
- [Contributing](#contributing)

---

## üöÄ Installation

### Prerequisites

1. **SUMO (Simulation of Urban MObility)**

    ```bash
    # macOS
    brew install sumo

    # Ubuntu/Debian
    sudo apt-get install sumo sumo-tools sumo-doc

    # Set SUMO_HOME environment variable
    export SUMO_HOME="/path/to/sumo"
    ```

2. **Python 3.9+**
    ```bash
    python --version  # Should be 3.9 or higher
    ```

### Setup

1. **Clone the repository**

    ```bash
    git clone https://github.com/yourusername/SignalSyncPro.git
    cd SignalSyncPro
    ```

2. **Create virtual environment**

    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3. **Install dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Configure environment**
    ```bash
    cp .env.example .env
    # Edit .env if needed (optional)
    ```

---

## ‚ö° Quick Start

### Train a DRL Agent

```bash
# Start training (runs in background)
./scripts/drl/run/run_training.sh

# Monitor progress
tail -100f training.log
```

### Test Trained Agent

```bash
# Test on all 30 scenarios
./scripts/drl/run/run_testing.sh models/training_YYYYMMDD_HHMMSS/final_model.pth
```

### Test Baseline (Rule-Based Control)

```bash
# Test developed control on all scenarios
./scripts/rule_based/run_developed_test.sh
```

---

## üìÅ Project Structure

```
SignalSyncPro/
‚îú‚îÄ‚îÄ common/                      # Shared utilities
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                 # General utilities (traffic load calculation)
‚îÇ   ‚îî‚îÄ‚îÄ sumo_utils.py           # SUMO path setup utilities
‚îÇ
‚îú‚îÄ‚îÄ constants/                   # Configuration constants
‚îÇ   ‚îú‚îÄ‚îÄ developed/              # Developed control constants
‚îÇ   ‚îî‚îÄ‚îÄ reference/              # Reference control constants
‚îÇ
‚îú‚îÄ‚îÄ controls/                    # Traffic control implementations
‚îÇ   ‚îú‚îÄ‚îÄ ml_based/               # Machine learning controls
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ drl/                # Deep Reinforcement Learning
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ agent.py        # DQN Agent
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ config.py       # DRL configuration
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ reward.py       # Reward function
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ traffic_management.py  # Environment
‚îÇ   ‚îî‚îÄ‚îÄ rule_based/             # Rule-based controls
‚îÇ       ‚îú‚îÄ‚îÄ developed/          # Optimized rule-based control
‚îÇ       ‚îî‚îÄ‚îÄ reference/          # Baseline control
‚îÇ
‚îú‚îÄ‚îÄ infrastructure/              # SUMO network files
‚îÇ   ‚îú‚îÄ‚îÄ developed/              # Developed control infrastructure
‚îÇ   ‚îî‚îÄ‚îÄ reference/              # Reference control infrastructure
‚îÇ
‚îú‚îÄ‚îÄ route_generator/            # Traffic route generation
‚îÇ   ‚îú‚îÄ‚îÄ traffic_config.py       # Traffic configuration
‚îÇ   ‚îú‚îÄ‚îÄ developed/              # Route generators for developed control
‚îÇ   ‚îî‚îÄ‚îÄ reference/              # Route generators for reference control
‚îÇ
‚îú‚îÄ‚îÄ run/                        # Execution scripts
‚îÇ   ‚îú‚îÄ‚îÄ training/               # Training scripts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_drl.py        # Main training script
‚îÇ   ‚îî‚îÄ‚îÄ testing/                # Testing scripts
‚îÇ       ‚îú‚îÄ‚îÄ test_drl.py         # DRL testing
‚îÇ       ‚îî‚îÄ‚îÄ test_developed.py   # Rule-based testing
‚îÇ
‚îú‚îÄ‚îÄ scripts/                    # Shell scripts
‚îÇ   ‚îú‚îÄ‚îÄ drl/run/               # DRL execution scripts
‚îÇ   ‚îî‚îÄ‚îÄ rule_based/            # Rule-based execution scripts
‚îÇ
‚îú‚îÄ‚îÄ logs/                       # Training/testing logs
‚îú‚îÄ‚îÄ models/                     # Saved model checkpoints
‚îú‚îÄ‚îÄ results/                    # Test results and metrics
‚îî‚îÄ‚îÄ .env                        # Environment configuration
```

---

## üéì Training

### Training Process

The DRL agent learns through experience by:

1. **Observing** traffic state (queue lengths, waiting times, phase info)
2. **Taking actions** (advance phase, skip to P1, activate pedestrian phase)
3. **Receiving rewards** based on performance (waiting times, sync rate, safety)
4. **Learning** optimal policies through Q-learning

### Training Configuration

Key parameters in `constants/developed/common/constants.py`:

```python
NUM_EPISODES_TRAIN = 100        # Number of training episodes
SIMULATION_LIMIT_TRAIN = 3600   # Simulation duration (seconds)
EPSILON_START = 0.9             # Initial exploration rate
EPSILON_DECAY = 0.98            # Exploration decay rate
LEARNING_RATE = 0.0001          # Neural network learning rate
```

### Reward Function

The agent optimizes a weighted reward:

- **Waiting Time** (Œ±=6.0): Penalizes vehicle waiting
- **Sync Rate** (Œ±=0.15): Rewards coordinated signals
- **Pedestrian Demand** (Œ±=0.8): Responds to pedestrian needs
- **Safety**: Monitors headway, distance, and red light violations

### Monitoring Training

```bash
# Watch training progress
tail -100f training.log

# Check if training is running
ps aux | grep train_drl.py

# Stop training
kill <PID>
```

---

## üß™ Testing

### Test Scenarios

**30 predefined scenarios** for consistent evaluation:

- **Pr_0 to Pr_9**: Varying car volumes (100-1000/hr), constant bikes/peds (400/hr)
- **Bi_0 to Bi_9**: Varying bicycle volumes (100-1000/hr), constant cars/peds (400/hr)
- **Pe_0 to Pe_9**: Varying pedestrian volumes (100-1000/hr), constant cars/bikes (400/hr)

### Performance Metrics

- **Average Waiting Time**: Cars, bicycles, pedestrians
- **Sync Rate**: Percentage of synchronized signal changes
- **Pedestrian Phases**: Number of dedicated pedestrian phases activated
- **Safety Violations**: Headway, distance, and red light violations
- **CO2 Emissions**: Environmental impact

### Running Tests

```bash
# Test DRL agent
./scripts/drl/run/run_testing.sh models/training_20241019/final_model.pth

# Test rule-based control
./scripts/rule_based/run_developed_test.sh

# Results saved to:
# - results/drl_testing/
# - results/developed_testing/
```

---

## ‚öôÔ∏è Configuration

### Environment Variables (`.env`)

```bash
# SUMO GUI settings
SUMO_GUI=false              # Set to 'true' to visualize simulation

# Logging level
LOG_LEVEL=INFO              # DEBUG, INFO, WARNING, ERROR
```

### DRL Configuration

Edit `controls/ml_based/drl/config.py`:

```python
class DRLConfig:
    # Neural Network
    HIDDEN_LAYERS = [128, 128]
    LEARNING_RATE = 0.0001

    # Training
    BATCH_SIZE = 64
    GAMMA = 0.95
    MEMORY_SIZE = 10000

    # Exploration
    EPSILON_START = 0.9
    EPSILON_END = 0.005
    EPSILON_DECAY = 0.98
```

### Traffic Configuration

Modify `route_generator/traffic_config.py` for custom scenarios:

```python
# Training: Random volumes
config = get_traffic_config(mode='training')

# Testing: Specific scenario
config = get_traffic_config(mode='test', scenario='Pr_5')
```

---

## üìä Results

### Expected Performance

**Target Metrics** (after 100 episodes):

- Car waiting time: 20-30s average
- Bicycle waiting time: 15-25s average
- Pedestrian phases: 150-250 per episode
- Sync rate: 60-70%
- Episode rewards: -0.5 to +0.3

### Comparison with Baseline

The DRL agent typically outperforms rule-based control by:

- **15-25%** reduction in average waiting times
- **Better adaptation** to varying traffic conditions
- **Higher sync rates** for coordinated flow

---

## üèóÔ∏è Architecture

### Key Components

1. **DQN Agent** (`controls/ml_based/drl/agent.py`)

    - Neural network with experience replay
    - Œµ-greedy exploration strategy
    - Target network for stable learning

2. **Traffic Management** (`controls/ml_based/drl/traffic_management.py`)

    - SUMO environment wrapper
    - State observation and action execution
    - Reward calculation and safety monitoring

3. **Route Generator** (`route_generator/`)

    - Dynamic traffic generation
    - Multi-modal support (cars, bikes, buses, pedestrians)
    - Configurable traffic volumes

4. **Reward Function** (`controls/ml_based/drl/reward.py`)
    - Multi-objective optimization
    - Weighted components for different priorities
    - Safety violation penalties

### Design Principles

- **Modularity**: Clean separation of concerns
- **Reusability**: Shared utilities and configurations
- **Extensibility**: Easy to add new controls or scenarios
- **Maintainability**: Well-documented code with type hints

---

## ü§ù Contributing

Contributions are welcome! Please follow these guidelines:

1. **Fork the repository**
2. **Create a feature branch** (`git checkout -b feature/amazing-feature`)
3. **Commit your changes** (`git commit -m 'Add amazing feature'`)
4. **Push to the branch** (`git push origin feature/amazing-feature`)
5. **Open a Pull Request**

### Code Style

- Follow PEP 8 guidelines
- Use `snake_case` for variables and functions
- Add docstrings to all functions
- Include type hints where appropriate

---

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **SUMO**: Eclipse SUMO traffic simulation platform
- **PyTorch**: Deep learning framework
- **Research**: Based on DRL traffic control research

---

## üìß Contact

For questions or feedback, please open an issue on GitHub.

---

**Built with ‚ù§Ô∏è for smarter traffic management**

---

---

# DRL Agent State Structure (VERIFIED FROM CODE)

## State Dimension: 32 features

### Structure (2 Intersections √ó 16 features each)

#### **TLS 3 (Intersection 1) - Features 0-15:**

| Index | Feature Name          | Description                                       | Range  |
| ----- | --------------------- | ------------------------------------------------- | ------ |
| 0     | TLS3_Phase_P1         | One-hot: Phase 1 active                           | 0 or 1 |
| 1     | TLS3_Phase_P2         | One-hot: Phase 2 active                           | 0 or 1 |
| 2     | TLS3_Phase_P3         | One-hot: Phase 3 active                           | 0 or 1 |
| 3     | TLS3_Phase_P4         | One-hot: Phase 4 active                           | 0 or 1 |
| 4     | TLS3_Phase_Duration   | Current phase duration (normalized)               | 0-1    |
| 5     | TLS3_Vehicle_Det1_P\* | Vehicle detector 1 for current phase P\* (binary) | 0 or 1 |
| 6     | TLS3_Vehicle_Det2_P\* | Vehicle detector 2 for current phase P\* (binary) | 0 or 1 |
| 7     | TLS3_Vehicle_Det3_P\* | Vehicle detector 3 for current phase P\* (binary) | 0 or 1 |
| 8     | TLS3_Vehicle_Det4_P\* | Vehicle detector 4 for current phase P\* (binary) | 0 or 1 |
| 9     | TLS3_Bicycle_Det1_P\* | Bicycle detector 1 for current phase P\* (binary) | 0 or 1 |
| 10    | TLS3_Bicycle_Det2_P\* | Bicycle detector 2 for current phase P\* (binary) | 0 or 1 |
| 11    | TLS3_Bicycle_Det3_P\* | Bicycle detector 3 for current phase P\* (binary) | 0 or 1 |
| 12    | TLS3_Bicycle_Det4_P\* | Bicycle detector 4 for current phase P\* (binary) | 0 or 1 |
| 13    | TLS3_Bus_Present      | Bus presence (binary)                             | 0 or 1 |
| 14    | TLS3_Bus_Wait         | Bus wait time (normalized)                        | 0-1    |
| 15    | TLS3_Sim_Time         | Simulation time (normalized)                      | 0-1    |

#### **TLS 6 (Intersection 2) - Features 16-31:**

| Index | Feature Name          | Description                                       | Range  |
| ----- | --------------------- | ------------------------------------------------- | ------ |
| 16    | TLS6_Phase_P1         | One-hot: Phase 1 active                           | 0 or 1 |
| 17    | TLS6_Phase_P2         | One-hot: Phase 2 active                           | 0 or 1 |
| 18    | TLS6_Phase_P3         | One-hot: Phase 3 active                           | 0 or 1 |
| 19    | TLS6_Phase_P4         | One-hot: Phase 4 active                           | 0 or 1 |
| 20    | TLS6_Phase_Duration   | Current phase duration (normalized)               | 0-1    |
| 21    | TLS6_Vehicle_Det1_P\* | Vehicle detector 1 for current phase P\* (binary) | 0 or 1 |
| 22    | TLS6_Vehicle_Det2_P\* | Vehicle detector 2 for current phase P\* (binary) | 0 or 1 |
| 23    | TLS6_Vehicle_Det3_P\* | Vehicle detector 3 for current phase P\* (binary) | 0 or 1 |
| 24    | TLS6_Vehicle_Det4_P\* | Vehicle detector 4 for current phase P\* (binary) | 0 or 1 |
| 25    | TLS6_Bicycle_Det1_P\* | Bicycle detector 1 for current phase P\* (binary) | 0 or 1 |
| 26    | TLS6_Bicycle_Det2_P\* | Bicycle detector 2 for current phase P\* (binary) | 0 or 1 |
| 27    | TLS6_Bicycle_Det3_P\* | Bicycle detector 3 for current phase P\* (binary) | 0 or 1 |
| 28    | TLS6_Bicycle_Det4_P\* | Bicycle detector 4 for current phase P\* (binary) | 0 or 1 |
| 29    | TLS6_Bus_Present      | Bus presence (binary)                             | 0 or 1 |
| 30    | TLS6_Bus_Wait         | Bus wait time (normalized)                        | 0-1    |
| 31    | TLS6_Sim_Time         | Simulation time (normalized)                      | 0-1    |

---

## Important: Phase-Specific Detectors

The 4 vehicle detectors (features 5-8, 21-24) and 4 bicycle detectors (features 9-12, 25-28) are **phase-dependent**.

Each phase has its own set of 4 detectors monitoring different approaches:

- **P1 detectors:** Edges 2‚Üí3, 4‚Üí3, 5‚Üí6, 7‚Üí6
- **P2 detectors:** Different set of 4 approaches
- **P3 detectors:** Different set of 4 approaches
- **P4 detectors:** Different set of 4 approaches

When the agent observes the state, the detector values correspond to **whichever phase is currently active** (indicated
by the one-hot encoding in features 0-3 or 16-19).

**Example:**

- If Phase P1 is active (feature [0]=1), then features [5-8] contain queue status for P1's 4 vehicle detectors
- If Phase P2 is active (feature [1]=1), then features [5-8] contain queue status for P2's 4 vehicle detectors

**Source:** `detectors/developed/drl/detectors.py` and `traffic_management.py::_get_detector_queues()`

---

## Action Space: 3 actions

| Action | Name     | Description                       |
| ------ | -------- | --------------------------------- |
| 0      | Continue | Continue current phase            |
| 1      | Skip2P1  | Skip to Phase 1 (major arterial)  |
| 2      | Next     | Advance to next phase in sequence |

---

## Phase Structure: 4 Controllable Phases

| Phase ID | Name        | Description                                    | Max Green Time |
| -------- | ----------- | ---------------------------------------------- | -------------- |
| 1        | PHASE_ONE   | Major roadway through + permissible right turn | 44s            |
| 5        | PHASE_TWO   | Major roadway protected left turn              | 15s            |
| 9        | PHASE_THREE | Minor roadway through + permissible right turn | 24s            |
| 13       | PHASE_FOUR  | Minor roadway protected left turn              | 12s            |

**Movement Types:**

- **Through + Permissible Right (P1, P3):** Car, bicycle, and pedestrian movements with green signal; right turns on
  permissible basis
- **Protected Left Turn (P2, P4):** Car left turns with protected green arrow; bicycles and pedestrians allowed but not
  protected (shown as dotted lines in signal diagram)

**Note:** There is NO dedicated pedestrian-only phase. All phases serve mixed traffic with varying protection levels.

---

## Code References

- State construction: `controls/ml_based/drl/traffic_management.py::_get_state()` (lines 113-141)
- Phase encoding: `controls/ml_based/drl/traffic_management.py::_encode_phase()` (lines 155-173)
- Configuration: `controls/ml_based/drl/config.py` (STATE_DIM=32, ACTION_DIM=3)
- Phase constants: `constants/developed/common/tls_constants.py`

---

---

<div align="center">
<img src="images/Deep_Learning_ND_P2_C_3_01.png" alt="Channel Attention" width="500" height=auto/>
<p align="center">figure: Channel Attention (Squeeze-and-excitation, or SE, block)</p>
</div>

---

---

## **10 Journal Recommendations (5 per Paper)**

---

# **Paper 1: Core DRL Traffic Control System**

## **üü¢ Easier Q1 Targets (Higher Acceptance):**

### **1. Applied Soft Computing**

- **Impact Factor:** 7.2
- **Quartile:** Q1
- **Acceptance Rate:** ~30%
- **Difficulty:** üü¢ EASIEST
- **Your Fit:** ‚≠ê‚≠ê‚≠ê‚≠ê EXCELLENT
- **Why:** Accepts applied AI/ML work, loves real-world validation
- **Focus:** Emphasize SUMO validation, multi-objective optimization
- **Typical Turnaround:** 4-6 months
- **URL:** https://www.sciencedirect.com/journal/applied-soft-computing

---

### **2. Engineering Applications of Artificial Intelligence**

- **Impact Factor:** 7.5
- **Quartile:** Q1
- **Acceptance Rate:** ~25%
- **Difficulty:** üü¢ EASY-MODERATE
- **Your Fit:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê PERFECT
- **Why:** Engineering + AI focus, practical applications valued
- **Focus:** Real-world constraints (MIN_GREEN, MAX_GREEN), safety
- **Typical Turnaround:** 5-7 months
- **URL:** https://www.sciencedirect.com/journal/engineering-applications-of-artificial-intelligence

---

## **üü° Moderate Q1 Targets (Competitive):**

### **3. Expert Systems with Applications**

- **Impact Factor:** 7.5
- **Quartile:** Q1
- **Acceptance Rate:** ~22%
- **Difficulty:** üü° MODERATE
- **Your Fit:** ‚≠ê‚≠ê‚≠ê‚≠ê VERY GOOD
- **Why:** Wide scope, accepts intelligent systems, good reputation
- **Focus:** Multi-agent coordination, decision-making framework
- **Typical Turnaround:** 6-8 months
- **URL:** https://www.sciencedirect.com/journal/expert-systems-with-applications

---

## **üî¥ Harder Q1 Targets (Top Tier):**

### **4. Transportation Research Part C: Emerging Technologies**

- **Impact Factor:** 7.6
- **Quartile:** Q1 (Top tier transportation)
- **Acceptance Rate:** ~18-20%
- **Difficulty:** üî¥ HARD
- **Your Fit:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê IDEAL VENUE
- **Why:** THE journal for innovative traffic control, AI/ML focus
- **Focus:** Novel DRL approach, comparison to state-of-practice
- **Typical Turnaround:** 8-12 months
- **URL:** https://www.sciencedirect.com/journal/transportation-research-part-c-emerging-technologies

---

### **5. IEEE Transactions on Intelligent Transportation Systems (TITS)**

- **Impact Factor:** 7.9
- **Quartile:** Q1 (Premier ITS journal)
- **Acceptance Rate:** ~15-18%
- **Difficulty:** üî¥ HARDEST
- **Your Fit:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê PERFECT BUT COMPETITIVE
- **Why:** Top ITS journal, highly competitive, best for career
- **Focus:** Multi-objective optimization, distributed coordination
- **Typical Turnaround:** 10-14 months
- **URL:** https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6979

---

# **Paper 2: Explainability/XAI for Traffic Control**

## **üü¢ Easier Q1 Targets (Higher Acceptance):**

### **1. IEEE Access**

- **Impact Factor:** 3.9
- **Quartile:** Q1 (Open Access)
- **Acceptance Rate:** ~35-40%
- **Difficulty:** üü¢ EASIEST
- **Your Fit:** ‚≠ê‚≠ê‚≠ê‚≠ê GOOD
- **Why:** Fast review, open access, broad scope
- **Focus:** Practical XAI tools for traffic engineers
- **Typical Turnaround:** 2-4 months (FAST!)
- **Fee:** ~$2,000 USD (open access)
- **URL:** https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6287639

---

### **2. Applied Intelligence**

- **Impact Factor:** 5.3
- **Quartile:** Q1
- **Acceptance Rate:** ~28%
- **Difficulty:** üü¢ EASY-MODERATE
- **Your Fit:** ‚≠ê‚≠ê‚≠ê‚≠ê VERY GOOD
- **Why:** Loves interpretability work, XAI papers welcome
- **Focus:** Multi-method XAI approach (VIPER + attention + saliency)
- **Typical Turnaround:** 4-6 months
- **URL:** https://www.springer.com/journal/10489

---

## **üü° Moderate Q1 Targets (Competitive):**

### **3. Expert Systems with Applications**

- **Impact Factor:** 7.5
- **Quartile:** Q1
- **Acceptance Rate:** ~22%
- **Difficulty:** üü° MODERATE
- **Your Fit:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê EXCELLENT
- **Why:** Publishes XAI work regularly, practitioner focus
- **Focus:** Decision tree extraction, actionable insights
- **Typical Turnaround:** 6-8 months
- **URL:** https://www.sciencedirect.com/journal/expert-systems-with-applications

---

## **üî¥ Harder Q1 Targets (Top Tier):**

### **4. IEEE Transactions on Neural Networks and Learning Systems (TNNLS)**

- **Impact Factor:** 10.4
- **Quartile:** Q1 (Top AI/ML journal)
- **Acceptance Rate:** ~12-15%
- **Difficulty:** üî¥ HARD
- **Your Fit:** ‚≠ê‚≠ê‚≠ê‚≠ê GOOD (need strong ML novelty)
- **Why:** Premier ML journal, interpretability is hot topic
- **Focus:** Novel XAI methodology, theoretical contribution
- **Typical Turnaround:** 10-14 months
- **URL:** https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385

---

### **5. Transportation Research Part C: Emerging Technologies**

- **Impact Factor:** 7.6
- **Quartile:** Q1
- **Acceptance Rate:** ~18-20%
- **Difficulty:** üî¥ HARDEST (for XAI paper)
- **Your Fit:** ‚≠ê‚≠ê‚≠ê‚≠ê VERY GOOD
- **Why:** Wants interpretability for real deployment
- **Focus:** Bridging AI and transportation practitioners
- **Typical Turnaround:** 8-12 months
- **URL:** https://www.sciencedirect.com/journal/transportation-research-part-c-emerging-technologies

---

# **Recommended Strategy:**

## **For Paper 1 (DRL System):**

### **Option A: Conservative (Higher Success)**

1st try: **Engineering Applications of AI** or **Applied Soft Computing**

- If rejected ‚Üí revise ‚Üí **Expert Systems with Applications**
- Success probability: ~60-70%

### **Option B: Ambitious (Best Impact)**

1st try: **IEEE TITS** or **TR Part C**

- If rejected ‚Üí revise ‚Üí **Expert Systems with Applications**
- If rejected again ‚Üí **Engineering Applications of AI**
- Success probability: ~40-50% first try, ~80% eventually

---

## **For Paper 2 (XAI/Explainability):**

### **Option A: Fast Publication**

1st try: **IEEE Access** (2-4 months, open access)

- High acceptance, good for CV
- Success probability: ~70-80%

### **Option B: Better Impact (Recommended)**

1st try: **Expert Systems with Applications**

- If rejected ‚Üí **Applied Intelligence**
- Success probability: ~50-60% first try, ~75% eventually

### **Option C: High Risk/High Reward**

1st try: **IEEE TNNLS** (need strong ML contribution)

- If rejected ‚Üí **Expert Systems with Applications**
- Success probability: ~30% first try

---

# **My Specific Recommendation:**

## **Paper 1:**

üéØ **Submit to: Transportation Research Part C**

- Best fit for your work
- Strong reputation in traffic control
- If rejected, pivot to Expert Systems with Applications

## **Paper 2:**

üéØ **Submit to: Expert Systems with Applications**

- Perfect fit for XAI work
- Good impact factor (7.5)
- Reasonable acceptance rate (~22%)
- If rejected, pivot to Applied Intelligence

---

# **Success Tips:**

### **For All Submissions:**

1. **Strong introduction** - hook readers in first paragraph
2. **Clear novelty statement** - what's NEW beyond "DRL works"
3. **Comprehensive related work** - cite 2023-2024 papers
4. **Statistical significance** - t-tests on your results
5. **High-quality figures** - professional matplotlib/seaborn plots
6. **Ablation studies** - show each component's impact

### **Emphasize Your Strengths:**

- ‚úÖ Multi-objective optimization (waiting + CO2 + equity + safety)
- ‚úÖ Zero safety violations (practical deployability)
- ‚úÖ 30 test scenarios (rigorous evaluation)
- ‚úÖ Comprehensive XAI (4 methods - rare in traffic domain!)
- ‚úÖ Real SUMO validation (not toy examples)

### **Address Weaknesses Proactively:**

- Acknowledge DRL is not novel by itself
- Focus on domain-specific contributions
- Emphasize practical constraints and real-world applicability

---

**Final Ranking by Difficulty:**

**Easiest ‚Üí Hardest:**

1. IEEE Access (Paper 2) - 35-40% acceptance
2. Applied Soft Computing (Paper 1) - 30%
3. Applied Intelligence (Paper 2) - 28%
4. EAAI (Paper 1) - 25%
5. Expert Systems (Both) - 22%
6. TR Part C (Both) - 18-20%
7. IEEE TITS (Paper 1) - 15-18%
8. IEEE TNNLS (Paper 2) - 12-15%

**Good luck! You have solid, publishable work!** üí™üìù


