# SignalSyncPro ğŸš¦

**Advanced Traffic Signal Control using Deep Reinforcement Learning**

SignalSyncPro is a sophisticated traffic management system that uses Deep Q-Network (DQN) reinforcement learning to
optimize traffic signal timing at intersections. The system learns to minimize vehicle waiting times while maintaining
safe and efficient traffic flow across multiple vehicle types (cars, bicycles, buses, and pedestrians).

<img src="images/simulation.png" width="400" height="auto" alt="SUMO Traffic Simulation">

---

## ğŸŒŸ Features

### **Intelligent Traffic Control**

- **Deep Reinforcement Learning**: DQN-based agent that learns optimal signal timing policies
- **Multi-Modal Support**: Handles cars, bicycles, buses, and pedestrians
- **Adaptive Learning**: Trains on diverse traffic scenarios for robust performance
- **Real-Time Optimization**: Minimizes waiting times while ensuring safety

### **Comprehensive Testing**

- **30 Test Scenarios**: Systematic evaluation across varying traffic conditions
- **Baseline Comparison**: Rule-based "developed" control for performance benchmarking
- **Detailed Metrics**: Tracks waiting times, sync rates, safety violations, and emissions

### **Production-Ready Architecture**

- **Modular Design**: Clean separation of concerns (controls, routes, infrastructure)
- **Reusable Utilities**: Centralized path setup and traffic configuration
- **Flexible Configuration**: Easy-to-modify constants and parameters
- **Comprehensive Logging**: Detailed training and testing logs

---

## ğŸ“‹ Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Training](#training)
- [Testing](#testing)
- [Configuration](#configuration)
- [Results](#results)
- [Architecture](#architecture)
- [Contributing](#contributing)

---

## ğŸš€ Installation

### Prerequisites

1. **SUMO (Simulation of Urban MObility)**

    ```bash
    # macOS
    brew install sumo

    # Ubuntu/Debian
    sudo apt-get install sumo sumo-tools sumo-doc

    # Set SUMO_HOME environment variable
    export SUMO_HOME="/path/to/sumo"
    ```

2. **Python 3.9+**
    ```bash
    python --version  # Should be 3.9 or higher
    ```

### Setup

1. **Clone the repository**

    ```bash
    git clone https://github.com/yourusername/SignalSyncPro.git
    cd SignalSyncPro
    ```

2. **Create virtual environment**

    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3. **Install dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Configure environment**
    ```bash
    cp .env.example .env
    # Edit .env if needed (optional)
    ```

---

## âš¡ Quick Start

### Train a DRL Agent

```bash
# Start training (runs in background)
./scripts/drl/run/run_training.sh

# Monitor progress
tail -100f training.log
```

### Test Trained Agent

```bash
# Test on all 30 scenarios
./scripts/drl/run/run_testing.sh models/training_YYYYMMDD_HHMMSS/final_model.pth
```

### Test Baseline (Rule-Based Control)

```bash
# Test developed control on all scenarios
./scripts/rule_based/run_developed_test.sh
```

---

## ğŸ“ Project Structure

```
SignalSyncPro/
â”œâ”€â”€ common/                      # Shared utilities
â”‚   â”œâ”€â”€ utils.py                 # General utilities (traffic load calculation)
â”‚   â””â”€â”€ sumo_utils.py           # SUMO path setup utilities
â”‚
â”œâ”€â”€ constants/                   # Configuration constants
â”‚   â”œâ”€â”€ developed/              # Developed control constants
â”‚   â””â”€â”€ reference/              # Reference control constants
â”‚
â”œâ”€â”€ controls/                    # Traffic control implementations
â”‚   â”œâ”€â”€ ml_based/               # Machine learning controls
â”‚   â”‚   â””â”€â”€ drl/                # Deep Reinforcement Learning
â”‚   â”‚       â”œâ”€â”€ agent.py        # DQN Agent
â”‚   â”‚       â”œâ”€â”€ config.py       # DRL configuration
â”‚   â”‚       â”œâ”€â”€ reward.py       # Reward function
â”‚   â”‚       â””â”€â”€ traffic_management.py  # Environment
â”‚   â””â”€â”€ rule_based/             # Rule-based controls
â”‚       â”œâ”€â”€ developed/          # Optimized rule-based control
â”‚       â””â”€â”€ reference/          # Baseline control
â”‚
â”œâ”€â”€ infrastructure/              # SUMO network files
â”‚   â”œâ”€â”€ developed/              # Developed control infrastructure
â”‚   â””â”€â”€ reference/              # Reference control infrastructure
â”‚
â”œâ”€â”€ route_generator/            # Traffic route generation
â”‚   â”œâ”€â”€ traffic_config.py       # Traffic configuration
â”‚   â”œâ”€â”€ developed/              # Route generators for developed control
â”‚   â””â”€â”€ reference/              # Route generators for reference control
â”‚
â”œâ”€â”€ run/                        # Execution scripts
â”‚   â”œâ”€â”€ training/               # Training scripts
â”‚   â”‚   â””â”€â”€ train_drl.py        # Main training script
â”‚   â””â”€â”€ testing/                # Testing scripts
â”‚       â”œâ”€â”€ test_drl.py         # DRL testing
â”‚       â””â”€â”€ test_developed.py   # Rule-based testing
â”‚
â”œâ”€â”€ scripts/                    # Shell scripts
â”‚   â”œâ”€â”€ drl/run/               # DRL execution scripts
â”‚   â””â”€â”€ rule_based/            # Rule-based execution scripts
â”‚
â”œâ”€â”€ logs/                       # Training/testing logs
â”œâ”€â”€ models/                     # Saved model checkpoints
â”œâ”€â”€ results/                    # Test results and metrics
â””â”€â”€ .env                        # Environment configuration
```

---

## ğŸ“ Training

### Training Process

The DRL agent learns through experience by:

1. **Observing** traffic state (queue lengths, waiting times, phase info)
2. **Taking actions** (advance phase, skip to P1, activate pedestrian phase)
3. **Receiving rewards** based on performance (waiting times, sync rate, safety)
4. **Learning** optimal policies through Q-learning

### Training Configuration

Key parameters in `constants/developed/common/constants.py`:

```python
NUM_EPISODES_TRAIN = 100        # Number of training episodes
SIMULATION_LIMIT_TRAIN = 3600   # Simulation duration (seconds)
EPSILON_START = 0.9             # Initial exploration rate
EPSILON_DECAY = 0.98            # Exploration decay rate
LEARNING_RATE = 0.0001          # Neural network learning rate
```

### Reward Function

The agent optimizes a weighted reward:

- **Waiting Time** (Î±=6.0): Penalizes vehicle waiting
- **Sync Rate** (Î±=0.15): Rewards coordinated signals
- **Pedestrian Demand** (Î±=0.8): Responds to pedestrian needs
- **Safety**: Monitors headway, distance, and red light violations

### Monitoring Training

```bash
# Watch training progress
tail -100f training.log

# Check if training is running
ps aux | grep train_drl.py

# Stop training
kill <PID>
```

---

## ğŸ§ª Testing

### Test Scenarios

**30 predefined scenarios** for consistent evaluation:

- **Pr_0 to Pr_9**: Varying car volumes (100-1000/hr), constant bikes/peds (400/hr)
- **Bi_0 to Bi_9**: Varying bicycle volumes (100-1000/hr), constant cars/peds (400/hr)
- **Pe_0 to Pe_9**: Varying pedestrian volumes (100-1000/hr), constant cars/bikes (400/hr)

### Performance Metrics

- **Average Waiting Time**: Cars, bicycles, pedestrians
- **Sync Rate**: Percentage of synchronized signal changes
- **Pedestrian Phases**: Number of dedicated pedestrian phases activated
- **Safety Violations**: Headway, distance, and red light violations
- **CO2 Emissions**: Environmental impact

### Running Tests

```bash
# Test DRL agent
./scripts/drl/run/run_testing.sh models/training_20241019/final_model.pth

# Test rule-based control
./scripts/rule_based/run_developed_test.sh

# Results saved to:
# - results/drl_testing/
# - results/developed_testing/
```

---

## âš™ï¸ Configuration

### Environment Variables (`.env`)

```bash
# SUMO GUI settings
SUMO_GUI=false              # Set to 'true' to visualize simulation

# Logging level
LOG_LEVEL=INFO              # DEBUG, INFO, WARNING, ERROR
```

### DRL Configuration

Edit `controls/ml_based/drl/config.py`:

```python
class DRLConfig:
    # Neural Network
    HIDDEN_LAYERS = [128, 128]
    LEARNING_RATE = 0.0001

    # Training
    BATCH_SIZE = 64
    GAMMA = 0.95
    MEMORY_SIZE = 10000

    # Exploration
    EPSILON_START = 0.9
    EPSILON_END = 0.005
    EPSILON_DECAY = 0.98
```

### Traffic Configuration

Modify `route_generator/traffic_config.py` for custom scenarios:

```python
# Training: Random volumes
config = get_traffic_config(mode='training')

# Testing: Specific scenario
config = get_traffic_config(mode='test', scenario='Pr_5')
```

---

## ğŸ“Š Results

### Expected Performance

**Target Metrics** (after 100 episodes):

- Car waiting time: 20-30s average
- Bicycle waiting time: 15-25s average
- Pedestrian phases: 150-250 per episode
- Sync rate: 60-70%
- Episode rewards: -0.5 to +0.3

### Comparison with Baseline

The DRL agent typically outperforms rule-based control by:

- **15-25%** reduction in average waiting times
- **Better adaptation** to varying traffic conditions
- **Higher sync rates** for coordinated flow

---

## ğŸ—ï¸ Architecture

### Key Components

1. **DQN Agent** (`controls/ml_based/drl/agent.py`)

    - Neural network with experience replay
    - Îµ-greedy exploration strategy
    - Target network for stable learning

2. **Traffic Management** (`controls/ml_based/drl/traffic_management.py`)

    - SUMO environment wrapper
    - State observation and action execution
    - Reward calculation and safety monitoring

3. **Route Generator** (`route_generator/`)

    - Dynamic traffic generation
    - Multi-modal support (cars, bikes, buses, pedestrians)
    - Configurable traffic volumes

4. **Reward Function** (`controls/ml_based/drl/reward.py`)
    - Multi-objective optimization
    - Weighted components for different priorities
    - Safety violation penalties

### Design Principles

- **Modularity**: Clean separation of concerns
- **Reusability**: Shared utilities and configurations
- **Extensibility**: Easy to add new controls or scenarios
- **Maintainability**: Well-documented code with type hints

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these guidelines:

1. **Fork the repository**
2. **Create a feature branch** (`git checkout -b feature/amazing-feature`)
3. **Commit your changes** (`git commit -m 'Add amazing feature'`)
4. **Push to the branch** (`git push origin feature/amazing-feature`)
5. **Open a Pull Request**

### Code Style

- Follow PEP 8 guidelines
- Use `snake_case` for variables and functions
- Add docstrings to all functions
- Include type hints where appropriate

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **SUMO**: Eclipse SUMO traffic simulation platform
- **PyTorch**: Deep learning framework
- **Research**: Based on DRL traffic control research

---

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

---

**Built with â¤ï¸ for smarter traffic management**
