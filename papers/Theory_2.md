# Deep Q-Network Architecture

The DRL agent employs a **Double Deep Q-Network (Double DQN)** with target network stabilization and Prioritized
Experience Replay (PER) for stable and efficient learning of multimodal traffic signal control policies.

##### Q-Function Approximation

The action-value function is approximated by a deep neural network:

$$
Q(s, a; \theta) : \mathbb{R}^{32} \times \mathcal{A} \to \mathbb{R}
$$

where $\theta$ represents the network parameters and $\mathcal{A} = {a_0, a_1, a_2}$ is the discrete action space.

**Function Interpretation:**

For a given traffic state $s \in \mathbb{R}^{32}$, the network outputs three Q-values:

$$
Q(s; \theta) = \begin{bmatrix} Q(s, a_0; \theta) \ Q(s, a_1; \theta) \ Q(s, a_2; \theta) \end{bmatrix}
$$

representing expected cumulative rewards for each action (Continue, Skip to P1, Next).

##### Network Architecture

The Q-network employs a **progressive expansion-compression** architecture with fully connected layers and ReLU
activations:

$$
\begin{align}
    &\text{Input Layer: } 32 \text{ dimensions (traffic state)} \\
    &\text{Hidden Layer 1: } 256 \text{ units, ReLU} \\
    &\text{Hidden Layer 2: } 256 \text{ units, ReLU} \\
    &\text{Hidden Layer 3: } 128 \text{ units, ReLU} \\
    &\text{Output Layer: } 3 \text{ units (Q-values for each action)} \\
\end{align}
$$

##### Parameter Count Calculation

$$
\begin{align}
    \text{Layer 1:} \quad & (32 \times 256) + 256 = 8{,}448 \\
    \text{Layer 2:} \quad & (256 \times 256) + 256 = 65{,}792 \\
    \text{Layer 3:} \quad & (256 \times 128) + 128 = 32{,}896 \\
    \text{Output:} \quad & (128 \times 3) + 3 = 387 \\
    \hline
    \text{Total:} \quad & 107{,}523 \text{ parameters}
\end{align}
$$

##### Architecture Design Rationale

**Expansion Phase (32 ‚Üí 256):**

- Captures complex traffic patterns and modal interactions
- Projects low-dimensional state into high-dimensional feature space
- Enables learning of nonlinear state-action relationships

**Maintenance Phase (256 ‚Üí 256):**

- Preserves representational capacity
- Allows deep feature extraction
- Facilitates gradient flow through network depth

**Compression Phase (256 ‚Üí 128 ‚Üí 3):**

- Distills features into action-value estimates
- Reduces dimensionality for efficient Q-value computation
- Prevents overfitting through parameter reduction

##### Forward Pass

The network computes Q-values through successive affine transformations and nonlinear activations:

$$
\begin{align}
    \mathbf{h}_1 &= \text{ReLU}(\mathbf{W}_1 \mathbf{s} + \mathbf{b}_1) \quad &\mathbf{h}_1 \in \mathbb{R}^{256} \\
    \mathbf{h}_2 &= \text{ReLU}(\mathbf{W}_2 \mathbf{h}_1 + \mathbf{b}_2) \quad &\mathbf{h}_2 \in \mathbb{R}^{256} \\
    \mathbf{h}_3 &= \text{ReLU}(\mathbf{W}_3 \mathbf{h}_2 + \mathbf{b}_3) \quad &\mathbf{h}_3 \in \mathbb{R}^{128} \\
    \mathbf{Q}(s; \theta) &= \mathbf{W}_4 \mathbf{h}_3 + \mathbf{b}_4 \quad &\mathbf{Q} \in \mathbb{R}^{3}
\end{align}
$$

where:

- $\mathbf{W}_i$ and $\mathbf{b}_i$ are learnable weight matrices and bias vectors
- $\text{ReLU}(x) = \max(0, x)$ introduces nonlinearity
- No activation on output layer (raw Q-value predictions)

##### Action Selection Strategy

##### Training Phase: $\epsilon$-Greedy Exploration

During training, the agent balances exploration (random actions) and exploitation (learned policy) using
$\epsilon$-greedy selection:

$$
a_t = \begin{cases} \text{random action from } \mathcal{A} & \text{with probability } \epsilon_t \\ \arg\max_{a \in \mathcal{A}} Q(s_t, a; \theta) & \text{with probability } 1 - \epsilon_t \end{cases}
$$

**Exploration Rate Decay:**

The exploration probability decreases exponentially over episodes:

$$
\epsilon_t = \max(\epsilon_{end}, \epsilon_{start} \times \gamma_{\epsilon}^t)
$$

where:

- $\epsilon_{start} = 1.0$: Initial full exploration (100% random actions)
- $\epsilon_{end} = 0.05$: Final exploration rate (5% random, 95% policy)
- $\gamma_{\epsilon} = 0.98$: Exponential decay factor (per episode)

**Decay Schedule:**

| Episode | $\epsilon_t$ | Exploration % | Exploitation % |
| ------- | ------------ | ------------- | -------------- |
| 1       | 1.00         | 100%          | 0%             |
| 10      | 0.82         | 82%           | 18%            |
| 50      | 0.36         | 36%           | 64%            |
| 100     | 0.13         | 13%           | 87%            |
| 150     | 0.05         | 5%            | 95%            |

##### Testing Phase: Greedy Policy

During evaluation, the agent uses purely greedy action selection:

$$
a_t = \arg\max_{a \in \mathcal{A}} Q(s_t, a; \theta^*)
$$

where $\theta^*$ represents the learned parameters after training.

##### Prioritized Experience Replay (PER)

##### Memory Buffer Structure

Experiences are stored as tuples in a circular buffer:

$$
e_t = (s_t, a_t, r_t, s_{t+1}, d_t, \text{info}_t)
$$

where:

- $s_t \in \mathbb{R}^{32}$: Current state
- $a_t \in {0, 1, 2}$: Action taken
- $r_t \in [-10, +10]$: Reward received (clipped)
- $s_{t+1} \in \mathbb{R}^{32}$: Next state
- $d_t \in {0, 1}$: Episode termination flag
- $\text{info}_t$: Auxiliary information (safety violations, event types, etc.)

**Buffer Capacity:** 50,000 experiences (oldest experiences overwritten when full)

##### Priority Assignment

Each experience receives priority based on its learning value, measured by Temporal Difference (TD) error magnitude:

$$
p_i = (|\delta_i| + \epsilon_{PER})^\alpha
$$

where:

- $\delta_i$: TD error for experience $i$
- $\epsilon_{PER} = 0.01$: Small constant preventing zero priority
- $\alpha = 0.6$: Prioritization strength (0 = uniform, 1 = proportional to TD error)

**Temporal Difference Error:**

$$
\delta_i = r_i + \gamma (1 - d_i) \max_{a'} Q(s_i', a'; \theta^-) - Q(s_i, a_i; \theta)
$$

Experiences with high TD error (large prediction mistakes) are sampled more frequently, accelerating learning from
surprising outcomes.

##### Sampling Probability

Experience $i$ is sampled with probability proportional to its priority:

$$
P(i) = \frac{p_i}{\sum_k p_k}
$$

**Sampling Strategy:**

1. Compute cumulative priority distribution: $C_k = \sum_{j=1}^{k} p_j$
2. Sample uniform random $u \sim \text{Uniform}(0, C_N)$
3. Select experience $i$ where $C_{i-1} < u \leq C_i$

This ensures experiences with high priority are sampled more frequently while maintaining stochastic diversity.

##### Importance Sampling Correction

Prioritized sampling introduces bias (experiences with high priority are overrepresented). Importance sampling (IS)
weights correct this bias:

$$
w_i = \left(\frac{1}{N \cdot P(i)}\right)^\beta
$$

where:

- $N$: Total experiences in buffer
- $P(i)$: Sampling probability of experience $i$
- $\beta$: Bias correction strength (anneals from 0.4 to 1.0)

**$\beta$ Annealing Schedule:**

$$
\beta_t = \min\left(1.0, \beta_{start} + \frac{t}{T_{frames}} \cdot (1.0 - \beta_{start})\right)
$$

where:

- $\beta_{start} = 0.4$: Initial bias correction (partial)
- $T_{frames} = 50{,}000$: Annealing duration (training steps)
- At $t = 0$: $\beta = 0.4$ (40% correction)
- At $t = 50{,}000$: $\beta = 1.0$ (100% correction, unbiased)

**Weight Normalization:**

To maintain training stability, IS weights are normalized by the maximum weight in the batch:

$$
w_i^{norm} = \frac{w_i}{\max_j w_j}
$$

ensuring $w_i^{norm} \in (0, 1]$ and preventing extreme weight magnitudes from destabilizing learning.

##### Double DQN Training Algorithm

##### Loss Function

The Q-network minimizes weighted mean squared Bellman error:

$$
\mathcal{L}(\theta) = \mathbb{E}_{(s,a,r,s',d,w) \sim \mathcal{B}} \left[w \cdot \mathcal{L}_{Huber}(\delta)\right]
$$

where $\mathcal{L}_{Huber}$ is the **Smooth L1 Loss** (robust to outliers):

$$
\mathcal{L}_{Huber}(\delta) = \begin{cases} \frac{1}{2}\delta^2 & \text{if } |\delta| < 1 \\ |\delta| - \frac{1}{2} & \text{otherwise} \end{cases}
$$

**Advantages over MSE:**

- Less sensitive to outlier TD errors
- More stable gradients for large errors
- Better convergence properties

##### Double DQN Target Computation

**Standard DQN (suffers from overestimation bias):**

$$
y_i = r_i + \gamma (1 - d_i) \max_{a'} Q(s_i', a'; \theta^-)
$$

The same network selects and evaluates the action, leading to systematic Q-value overestimation.

**Double DQN (reduces overestimation):**

$$
\begin{align}
a_{i}^{*} &= \arg\max_{a'} Q(s_{i}', a'; \theta) \quad \text{(policy network selects)} \\
y_{i} &= r_{i} + \gamma (1 - d_{i}) \cdot Q(s_{i}', a_{i}^{*}; \theta^{-}) \quad \text{(target network evaluates)}
\end{align}
$$

**Separation of Concerns:**

- **Policy network** ($\theta$): Selects best action based on current policy
- **Target network** ($\theta^-$): Evaluates selected action's value

This decoupling prevents overoptimistic Q-value estimation.

##### Temporal Difference Error

$$
\delta_i = y_i - Q(s_i, a_i; \theta)
$$

##### Gradient Descent Update

Parameters are updated using the Adam optimizer:

$$
\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}(\theta)
$$

with learning rate $\eta = 1 \times 10^{-5}$.

**Adam Optimizer Benefits:**

- Adaptive learning rates per parameter
- Momentum-based updates for faster convergence
- Robust to gradient noise and sparse gradients

##### Target Network Soft Update

The target network is updated gradually via exponential moving average:

$$
\theta^- \leftarrow \tau \theta + (1 - \tau) \theta^-
$$

where:

- $\tau = 0.005$: Soft update coefficient (0.5% policy, 99.5% target)
- Applied **after every training step**

**Rationale:**

Soft updates provide smoother target stability than periodic hard updates (copying all weights). The slow-moving target
reduces oscillations and divergence during training.

##### Multi-Layer Gradient Clipping

To prevent training instability from extreme gradients, multiple clipping mechanisms are applied:

**1. Reward Clipping:**

$$
r_t = \text{clip}(r_t, -10.0, +10.0)
$$

**2. Q-Value Clipping:**

$$
Q_{next} = \text{clip}(Q(s', a'; \theta^-), -10.0, +10.0)
$$

**3. Target Clipping:**

$$
y_i = \text{clip}(y_i, -10.0, +10.0)
$$

**4. Gradient Norm Clipping:**

$$
\nabla_\theta \mathcal{L} \leftarrow \begin{cases} \nabla_\theta \mathcal{L} & \text{if } |\nabla_\theta \mathcal{L}| \leq 0.5 \\ 0.5 \cdot \frac{\nabla_\theta \mathcal{L}}{|\nabla_\theta \mathcal{L}|} & \text{otherwise} \end{cases}
$$

These clipping strategies ensure numerical stability and prevent catastrophic gradient explosions.

---

---

# Design of Deep Learning Training

##### Training Data Generation

The DRL agent was trained using **stochastic traffic demand** to ensure robust policy learning across diverse
conditions. Each training episode generated random traffic volumes uniformly sampled from realistic ranges:

**Modal Demand Ranges:**

- **Private vehicles:** 100-1000 vehicles/hour
- **Bicycles:** 100-1000 bicycles/hour
- **Pedestrians:** 100-1000 pedestrians/hour
- **Buses:** Fixed frequency (every 15 minutes, 4 buses/hour)

**Validation During Training:**

To monitor learning progress, one of the 30 standardized test scenarios (Pr_0-9, Bi_0-9, Pe_0-9) was randomly selected
and evaluated approximately **every 4 episode** (without sequential order). This periodic validation enabled:

- Early detection of training instabilities
- Convergence monitoring across scenario types
- Performance tracking on fixed benchmarks while training on stochastic demand

**Training Duration:** 200 episodes, each simulating 3,600 seconds (1 hour) of traffic operation.

**Rationale:** The randomized demand generation prevents overfitting to specific traffic patterns and ensures the
learned policy generalizes to the full spectrum of operational conditions encountered during testing. The observed
reward fluctuations across episodes reflect this intentional diversity rather than training instability.

##### Training Episode Structure

##### Algorithmic Procedure

$$
\begin{align} &\textbf{Initialize:} \\ &\quad \text{Policy network } Q(s, a; \theta) \text{ with random weights} \\ &\quad \text{Target network } Q(s, a; \theta^-) \leftarrow \theta \\ &\quad \text{Replay buffer } \mathcal{D} \text{ with capacity } 50{,}000 \\ &\quad \text{Exploration rate } \epsilon \leftarrow 1.0 \\ \\[0.5em] &\textbf{For } \text{episode} = 1 \text{ to } N_{\text{episodes}}: \\ &\quad 1. \text{ Generate random traffic demand (cars: 100-1000/hr, bikes: 100-1000/hr, peds: 100-1000/hr)} \\ &\quad 2. \text{ Reset SUMO environment: } s_0 \leftarrow \text{env.reset}() \\ &\quad 3. \text{ Initialize both intersections to Phase 1 (leading green)} \\ \\[0.3em] &\quad 4. \text{ For timestep } t = 0 \text{ to } T_{\max} = 3{,}600: \\ &\quad\quad \text{a. Select action: } a_t \sim \begin{cases} \text{Uniform}(\mathcal{A}) & \text{w.p. } \epsilon \\ \arg\max_a Q(s_t, a; \theta) & \text{w.p. } 1-\epsilon \end{cases} \\ \\[0.3em] &\quad\quad \text{b. Execute action centrally (both intersections change simultaneously)} \\ &\quad\quad \text{c. Observe: } s_{t+1}, r_t, d_t, \text{info}_t \leftarrow \text{env.step}(a_t) \\ &\quad\quad \text{d. Compute TD error: } \delta_t = r_t + \gamma \max_{a'} Q(s_{t+1}, a'; \theta^-) - Q(s_t, a_t; \theta) \\ &\quad\quad \text{e. Store: } \mathcal{D}.\text{add}(s_t, a_t, r_t, s_{t+1}, d_t, \text{info}_t) \\ \\[0.3em] &\quad\quad \text{f. If } |\mathcal{D}| \geq 1{,}000: \\ &\quad\quad\quad \text{i. Sample prioritized batch: } {(s_i, a_i, r_i, s_i', d_i, w_i)}_{i=1}^{64} \sim \mathcal{D} \\ &\quad\quad\quad \text{ii. Compute Double DQN targets: } y_i = r_i + \gamma (1-d_i) Q(s_i', \arg\max_{a'} Q(s_i', a'; \theta); \theta^-) \\ &\quad\quad\quad \text{iii. Compute weighted loss: } \mathcal{L}(\theta) = \frac{1}{64}\sum_{i=1}^{64} w_i \cdot \mathcal{L}_{Huber}(y_i - Q(s_i, a_i; \theta)) \\ &\quad\quad\quad \text{iv. Update policy network: } \theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}(\theta) \\ &\quad\quad\quad \text{v. Soft update target: } \theta^- \leftarrow 0.005 \theta + 0.995 \theta^- \\ &\quad\quad\quad \text{vi. Update priorities: } p_i \leftarrow (|\delta_i| + 0.01)^{0.6} \\ \\[0.3em] &\quad\quad \text{g. If } d_t = 1: \text{ break (episode done)} \\ \\[0.3em] &\quad 5. \text{ Decay exploration: } \epsilon \leftarrow \max(0.05, 0.98 \times \epsilon) \\ &\quad 6. \text{ Log episode metrics (rewards, waiting times, safety violations)} \\ &\quad 7. \text{ Save checkpoint every 10 episodes} \end{align}
$$

##### Hyperparameters

###### 1. Network Architecture

| Parameter           | Value           | Description                                    |
| ------------------- | --------------- | ---------------------------------------------- |
| Input dimensions    | 32              | State space size (16 per intersection)         |
| Hidden layers       | [256, 256, 128] | Progressive expansion-compression              |
| Output dimensions   | 3               | Action space size (Continue, Next, Skip to P1) |
| Activation function | ReLU            | $f(x) = \max(0, x)$                            |
| Total parameters    | 107,523         | Learnable weights and biases                   |

###### 2. Learning Parameters

| Parameter          | Symbol             | Value              | Description                |
| ------------------ | ------------------ | ------------------ | -------------------------- |
| Learning rate      | $\eta$             | $1 \times 10^{-5}$ | Adam optimizer step size   |
| Discount factor    | $\gamma$           | 0.95               | Future reward importance   |
| Exploration start  | $\epsilon_{start}$ | 1.0                | Initial exploration (100%) |
| Exploration end    | $\epsilon_{end}$   | 0.05               | Final exploration (5%)     |
| Exploration decay  | $\gamma_\epsilon$  | 0.98               | Per-episode decay rate     |
| Target soft update | $\tau$             | 0.005              | Target network blend rate  |
| Gradient clip norm | ‚Äî                  | 0.5                | Maximum gradient magnitude |

###### 3. Experience Replay

| Parameter          | Symbol           | Value  | Description                    |
| ------------------ | ---------------- | ------ | ------------------------------ |
| Buffer capacity    | $N$              | 50,000 | Maximum stored experiences     |
| Batch size         | ‚Äî                | 64     | Experiences sampled per update |
| Min buffer size    | ‚Äî                | 1,000  | Minimum before training starts |
| PER $\alpha$       | $\alpha$         | 0.6    | Prioritization exponent        |
| PER $\beta$ start  | $\beta_{start}$  | 0.4    | Initial IS correction          |
| PER $\beta$ frames | $T_{frames}$     | 50,000 | Annealing duration             |
| PER $\epsilon$     | $\epsilon_{PER}$ | 0.01   | Priority baseline              |

###### 4. Training Configuration

| Parameter            | Value             | Description                            |
| -------------------- | ----------------- | -------------------------------------- |
| Training episodes    | 200               | Number of training iterations          |
| Steps per episode    | 3,600             | 1-hour simulation (1s timesteps)       |
| Training triggers    | Every step        | After each env.step() if buffer ‚â• 1000 |
| Checkpoint frequency | Every 10 episodes | Model save interval                    |
| Convergence expected | 150+ episodes     | Typical learning duration              |

###### 5. Reward Component Weights

| Component      | Symbol               | Value | Purpose                      |
| -------------- | -------------------- | ----- | ---------------------------- |
| Waiting time   | $\alpha_{wait}$      | 2.5   | Primary efficiency metric    |
| Emissions      | $\alpha_{emission}$  | 0.05  | Environmental sustainability |
| Equity         | $\alpha_{equity}$    | 0.5   | Inter-modal fairness         |
| Safety         | $\alpha_{safety}$    | 2.0   | Critical constraint          |
| Blocked action | $\alpha_{block}$     | 0.1   | Execution efficiency         |
| Stability      | $\alpha_{stability}$ | 0.12  | Phase continuity             |
| Next bonus     | $\alpha_{next}$      | 2.0   | Multi-phase service          |

###### 6. Modal Priority Weights

| Mode       | Symbol           | Value | Rationale                                  |
| ---------- | ---------------- | ----- | ------------------------------------------ |
| Car        | $w_{car}$        | 1.3   | Moderate throughput priority               |
| Bicycle    | $w_{bicycle}$    | 1.0   | Vulnerable user baseline                   |
| Pedestrian | $w_{pedestrian}$ | 1.0   | Vulnerable user baseline                   |
| Bus        | $w_{bus}$        | 2.0   | Public transit priority (30-50 passengers) |

###### 7. Safety Thresholds

| Constraint         | Value | Standard/Justification   |
| ------------------ | ----- | ------------------------ |
| Min green (P1)     | 8s    | Major arterial clearance |
| Min green (P2)     | 3s    | Protected left turn      |
| Min green (P3)     | 5s    | Minor arterial clearance |
| Min green (P4)     | 2s    | Minor protected left     |
| Safe headway       | 2.0s  | 2-second rule (standard) |
| Collision distance | 5.0m  | Critical zone threshold  |

##### Training Algorithm Flowchart

```mermaid
flowchart TB
    Init["Initialize DQN Agent<br>Policy: Q(s,a;Œ∏), Target: Q(s,a;Œ∏‚Åª)<br>Replay Buffer: ùìë (capacity 50,000)<br>Œµ = 1.0, step = 0"]

    Episode["Start New Episode<br>Generate random traffic demand"]

    Reset["Reset SUMO Environment<br>s‚ÇÄ ‚Üê env.reset()<br>Both intersections ‚Üí Phase 1<br>t = 0"]

    Select["Select Action (Œµ-greedy)<br>Random (Œµ) or Policy (1-Œµ)<br>a‚Çú ~ Œµ-greedy(s‚Çú)"]

    Execute["Execute Action Centrally<br>Both intersections change together<br>s‚Çú‚Çä‚ÇÅ, r‚Çú, d‚Çú, info ‚Üê env.step(a‚Çú)"]

    Compute["Compute TD Error (Double DQN)<br>a* = argmax Q(s‚Çú‚Çä‚ÇÅ, a'; Œ∏)<br>Œ¥‚Çú = r‚Çú + Œ≥(1-d‚Çú)Q(s‚Çú‚Çä‚ÇÅ,a*;Œ∏‚Åª) - Q(s‚Çú,a‚Çú;Œ∏)"]

    Store["Store Experience in Buffer<br>ùìë.add(s‚Çú, a‚Çú, r‚Çú, s‚Çú‚Çä‚ÇÅ, d‚Çú, info)<br>Priority p‚Çú = (|Œ¥‚Çú| + 0.01)^0.6"]

    Check{"Buffer size<br>‚â• 1,000?"}

    Sample["Sample Prioritized Batch<br>B = {(s·µ¢,a·µ¢,r·µ¢,s·µ¢',d·µ¢,w·µ¢)}‚ÇÅ‚Å∂‚Å¥<br>Using priorities p·µ¢"]

    DQN["Compute Double DQN Targets<br>For each i:<br>a*·µ¢ = argmax Q(s·µ¢',a';Œ∏)<br>y·µ¢ = r·µ¢ + Œ≥(1-d·µ¢)Q(s·µ¢',a*·µ¢;Œ∏‚Åª)"]

    Loss["Compute Weighted Loss<br>‚Ñí(Œ∏) = (1/64)Œ£ w·µ¢¬∑Huber(y·µ¢ - Q(s·µ¢,a·µ¢;Œ∏))"]

    Update["Update Policy Network<br>Œ∏ ‚Üê Œ∏ - Œ∑‚àá‚Ñí(Œ∏)<br>Œ∑ = 1√ó10‚Åª‚Åµ<br>Clip gradients: ||‚àá|| ‚â§ 0.5"]

    Soft["Soft Update Target Network<br>Œ∏‚Åª ‚Üê 0.005Œ∏ + 0.995Œ∏‚Åª<br>(Every training step)"]

    UpdatePri["Update Buffer Priorities<br>For each i in batch:<br>p·µ¢ ‚Üê (|Œ¥·µ¢| + 0.01)^0.6"]

    Continue["Continue Episode<br>s‚Çú ‚Üê s‚Çú‚Çä‚ÇÅ<br>t ‚Üê t + 1<br>step ‚Üê step + 1"]

    Done{"Episode<br>done?<br>(t ‚â• 3600"}

    Decay["Decay Exploration<br>Œµ ‚Üê max(0.05, 0.98√óŒµ)"]

    Log["Log Episode Metrics<br>Total reward, waiting times,<br>safety violations, phase changes"]

    Save{"Episode<br>mod 10 = 0?"}

    Checkpoint["Save Model Checkpoint<br>Save Œ∏, Œ∏‚Åª, Œµ, step"]

    EpDone{"All episodes<br>complete?<br>(200 episodes)"}

    Finish["Training Complete<br>Save final model Œ∏*<br>Evaluate on test scenarios"]

    Init --> Episode
    Episode --> Reset
    Reset --> Select
    Select --> Execute
    Execute --> Compute
    Compute --> Store
    Store --> Check

    Check -->|No| Continue
    Check -->|Yes| Sample

    Sample --> DQN
    DQN --> Loss
    Loss --> Update
    Update --> Soft
    Soft --> UpdatePri
    UpdatePri --> Continue

    Continue --> Done

    Done -->|No| Select
    Done -->|Yes| Decay

    Decay --> Log
    Log --> Save

    Save -->|Yes| Checkpoint
    Save -->|No| EpDone
    Checkpoint --> EpDone

    EpDone -->|No| Episode
    EpDone -->|Yes| Finish

    style Init fill:#E3F2FD
    style Episode fill:#BBDEFB
    style Reset fill:#90CAF9
    style Select fill:#64B5F6
    style Execute fill:#42A5F5
    style Compute fill:#2196F3
    style Store fill:#1E88E5
    style Check fill:#1976D2
    style Sample fill:#81C784
    style DQN fill:#66BB6A
    style Loss fill:#4CAF50
    style Update fill:#388E3C
    style Soft fill:#2E7D32
    style UpdatePri fill:#1B5E20
    style Continue fill:#FFF59D
    style Done fill:#FFEB3B
    style Decay fill:#FDD835
    style Log fill:#FFE082
    style Save fill:#FFCA28
    style Checkpoint fill:#FFC107
    style EpDone fill:#FFB300
    style Finish fill:#FF6F00
```

##### 2-Phase Traning with Configuration Adjustments

Although the training loss exhibited steady convergence during the initial 100 episodes, indicating successful neural
network optimization, the agent's action selection patterns needs slight modification to align with the desired
operational profile for real-world traffic signal control. The target action distribution‚Äî85% Continue, 2.5%
Skip-to-Phase-1, and 12.5% Next Phase‚Äîrepresents theoretically optimal signal controller behavior where most decisions
maintain stable phases, occasional strategic returns to the main arterial phase occur when appropriate, and periodic
phase progressions serve accumulated demand. Analysis of Phase 1 training revealed suboptimal action frequency
distributions, prompting the reward function recalibration.

The Skip-to-Phase-1 incentive mechanism was refined by reducing both the penalty for inappropriate Continue actions and
the bonus for effective Skip-to-Phase-1 transitions, promoting more nuanced decision-making rather than oversimplified
action preferences. To encourage more stable traffic signal timing patterns, the stability reward component was
moderately increased, incentivizing the agent to maintain phases for appropriate durations rather than making frequent
rapid changes. Concurrently, the next-phase transition bonus was enhanced to better reward strategic phase progressions
that serve accumulated vehicle demand. These adjustments were motivated by observations from the first 100 episodes
where the agent exhibited excessive phase-switching behavior and underutilized certain beneficial actions. The
recalibrated configuration was then employed for an additional 100-episode training phase, with model 192 from this
second phase selected for subsequent testing based on its balanced performance across all traffic scenarios.

The observed reward fluctuations throughout both training phases reflect the intentional use of randomized traffic
generation (100-1000 vehicles per hour across different modes) to ensure robust policy learning across diverse traffic
conditions rather than overfitting to specific scenarios. This stochastic training environment, while introducing reward
variance, was essential for developing a generalizable controller capable of handling the unpredictable nature of
real-world traffic patterns encountered during testing.

##### Summary

This Deep Q-Network implementation provides a **robust, stable, and efficient** framework for learning multimodal
traffic signal control policies. Key architectural features include:

1. **Double DQN**: Reduces Q-value overestimation bias through action selection/evaluation separation
2. **Prioritized Experience Replay**: Accelerates learning by focusing on high-TD-error experiences
3. **Soft Target Updates**: Maintains training stability with gradual target network synchronization
4. **Multi-Layer Clipping**: Prevents training divergence through reward, Q-value, and gradient clipping
5. **Centralized Control**: Both intersections controlled simultaneously for natural coordination
6. **Phase-Aware Architecture**: Network learns phase-specific optimal durations and transitions
7. **Dense Reward Signal**: 14-component reward provides rich learning feedback at every timestep

The architecture balances **expressiveness** (107K parameters) with **computational efficiency** (~40 hours training on
GPU), enabling practical deployment for real-world traffic signal optimization.

---

---

# DRL Control Methodology

##### Control Architecture

This research implements a **centralized single-agent** Deep Q-Network (DQN) for coordinated traffic signal control
across two consecutive signalized intersections separated by 300 meters along a major arterial corridor.

**Centralized Control Strategy:**

The agent controls both traffic signals simultaneously - when the agent decides to change phase, both intersections
change to the same phase at the same time. This centralized approach is appropriate for closely-spaced intersections
(<600m) and naturally achieves coordination without requiring explicit synchronization algorithms or timing mechanisms.

**Key Design Decisions:**

- **No semi-synchronization needed**: Unlike rule-based controls that use timing offsets between intersections, both
  signals operate in perfect synchrony under a single agent's control
- **No exclusive pedestrian phase**: Pedestrians cross during regular vehicle phases (P1 and P3), rather than having a
  dedicated Phase 5 as in the thesis
- **Simplified phase structure**: 4-phase operation (P1-P4) for efficient traffic flow

The DRL agent learns optimal control policies through interaction with a high-fidelity SUMO (Simulation of Urban
MObility) traffic simulation environment, balancing multiple competing objectives including traffic efficiency, modal
equity, environmental sustainability, and safety.

```mermaid
flowchart TD
    A["üö¶ Traffic State Observation<br><b>32-Dimensional State Vector</b><br>Phase encoding: one-hot [4 dims]<br>Phase duration: normalized [1 dim]<br>Vehicle queues: 4 directions [4 dims]<br>Bicycle queues: 4 directions [4 dims]<br>Pedestrians: waiting count [1 dim]<br>Bus presence: binary [1 dim]<br>Bus waiting time: normalized [1 dim]<br>Per intersection √ó 2 = 32 total"] --> B["üß† Policy Network<br><b>32‚Üí256‚Üí256‚Üí128‚Üí3</b><br>Input: 32 neurons<br>Hidden 1: 256 + ReLU<br>Hidden 2: 256 + ReLU<br>Hidden 3: 128 + ReLU<br>Output: 3 Q-values<br>Parameters: 107,523"]

    B --> C["Q-Value Computation"]

    C --> D["Q(Continue) = 5.2"]
    C --> E["Q(Skip to P1) = 8.7"]
    C --> F["Q(Next) = 3.1"]

    D --> G["Action Selection<br>Œµ-greedy"]
    E --> G
    F --> G

    G -->|"Random"| H["üé≤ Exploration<br>Random action"]
    G -->|"Greedy"| I["‚úÖ Exploitation<br>argmax Q-value<br>Action: Skip to P1"]

    H --> J["‚öôÔ∏è Execute in SUMO<br>Centralized Control<br>Both intersections"]
    I --> J

    J --> K["üìä Multi-Objective Reward<br>14 Components<br>Waiting times per mode<br>Queue lengths<br>Phase penalties<br>Sync bonus: +10<br>Bus priority<br>Equity balance"]

    K --> L["üíæ Store in PER Buffer<br>Priority = TD-error<br>Buffer size: 100,000"]

    L --> M["Training Check<br>Buffer ‚â• 64<br>Step % 4 = 0"]

    M -->|"No"| N["Continue"]
    M -->|"Yes"| O["üìö Sample Batch<br>Size: 64<br>Prioritized sampling"]

    N --> A

    O --> P["üéØ Target Q-values<br>Double DQN<br>Œ≥ = 0.99<br>Clip [-10, 10]"]

    P --> Q["üìâ TD-Error<br>Update priorities<br>MSE Loss"]

    Q --> R["‚ö° Backpropagation<br>Adam: lr=0.0001<br>Update Œ∏_policy"]

    R --> S["Target Update<br>Steps % 500 = 0"]

    S -->|"No"| T["Continue"]
    S -->|"Yes"| U["üîÑ Soft Update<br>œÑ = 0.005"]

    T --> A
    U --> A

    V["üìà Decay Œµ<br>Œµ ‚Üê max(Œµ√ó0.98, 0.01)"] -.-> A

    style A fill:#E3F2FD
    style B fill:#C8E6C9
    style C fill:#FFF9C4
    style D fill:#E1F5FE
    style E fill:#FFE0B2
    style F fill:#E1F5FE
    style G fill:#81C784
    style H fill:#FFCCBC
    style I fill:#FFCCBC
    style J fill:#F48FB1
    style K fill:#CE93D8
    style L fill:#D1C4E9
    style M fill:#B2DFDB
    style N fill:#E0F2F1
    style O fill:#9C27B0
    style P fill:#7B1FA2
    style Q fill:#6A1B9A
    style R fill:#4A148C
    style S fill:#B2DFDB
    style T fill:#E0F2F1
    style U fill:#4CAF50
    style V fill:#FFF59D
```

##### Key Characteristics of the Approach

- **Centralized observation and control**: Single agent controls both intersections with global state awareness
- **Synchronized phase changes**: Both intersections always maintain the same phase simultaneously
- **Multi-objective optimization**: Balances seven competing objectives through weighted reward components
- **Traffic-responsive actuation**: Phase durations adapt to real-time traffic conditions
- **Modal equity**: Explicit priority weights for cars, bicycles, pedestrians, and public transit
- **Safety-critical learning**: Strong penalties for unsafe control actions

##### State Space Representation

The state space $\mathcal{S}$ provides the DRL agent with a comprehensive representation of current traffic conditions
at both intersections. The state vector $s_t \in \mathbb{R}^{32}$ combines traffic flow characteristics, signal phase
status, and temporal features.

##### State Vector Composition

For each intersection $i \in \{3, 6\}$, the state includes:

$$
s_t^{(i)} = [p^{(i)}, d^{(i)}, q_v^{(i)}, q_b^{(i)}, \phi_{ped}^{(i)}, \phi_{bus}^{(i)}]
$$

Where:

- $p^{(i)} \in \{0,1\}^4$: One-hot encoded phase (Phase 1-4)
- $d^{(i)} \in [0,1]$: Normalized phase duration
- $q_v^{(i)} \in [0,1]^4$: Vehicle queue occupancy (4 approaches)
- $q_b^{(i)} \in [0,1]^4$: Bicycle queue occupancy (4 approaches)
- $\phi_{ped}^{(i)} \in \{0,1\}$: Pedestrian demand indicator
- $\phi_{bus}^{(i)} \in \{0,1\}$: Bus presence indicator

**Complete state vector:**

$$
s_t = [s_t^{(3)}, s_t^{(6)}] \in \mathbb{R}^{32}
$$

**Dimensionality breakdown:**

- Intersection 3: 16 dimensions (4 + 1 + 4 + 4 + 1 + 1 + 1)
- Intersection 6: 16 dimensions (4 + 1 + 4 + 4 + 1 + 1 + 1)
- **Total: 32 dimensions**

##### Phase Encoding

The phase encoding represents the current traffic signal phase at each intersection. Since both intersections operate
synchronously under centralized control, they always maintain the same phase. The agent uses a 4-phase structure:

$$
p^{(i)} = \begin{cases}
[1,0,0,0] & \text{if Phase 1 (Major arterial through)} \\
[0,1,0,0] & \text{if Phase 2 (Major protected left turn)} \\
[0,0,1,0] & \text{if Phase 3 (Minor arterial through)} \\
[0,0,0,1] & \text{if Phase 4 (Minor protected left turn)}
\end{cases}
$$

**SUMO Phase Mapping:**

SUMO uses 16 internal phase indices for each intersection, which map to our 4 conceptual phases:

| Conceptual Phase | SUMO Indices   | Description                 | Typical Duration |
| ---------------- | -------------- | --------------------------- | ---------------- |
| **Phase 1**      | 0, 1, 2, 3     | Major through + right turns | 8-44 seconds     |
| **Phase 2**      | 4, 5, 6, 7     | Major protected left        | 3-15 seconds     |
| **Phase 3**      | 8, 9, 10, 11   | Minor through + right turns | 5-24 seconds     |
| **Phase 4**      | 12, 13, 14, 15 | Minor protected left        | 2-12 seconds     |

Each conceptual phase includes: leading green (1s) ‚Üí main green (variable) ‚Üí yellow (3s) ‚Üí all-red (2s)

##### Queue Detection

Queue occupancy is measured using induction loop detectors positioned 30 meters for cars and 15 meters for bicycles
upstream of stop lines for each approach. Binary occupancy indicates whether vehicles are waiting:

$$
q_j^{(i)} = \begin{cases}
1.0 & \text{if detector occupied within last 3.0 seconds} \\
0.0 & \text{otherwise}
\end{cases}
$$

**Detector Configuration:**

- **Vehicle detectors (D30)**: Positioned 30m upstream, window = 3.0s
- **Bicycle detectors (D15)**: Positioned 15m upstream, window = 3.0s
- **Four approaches per intersection**: North, South, East, West

The 3-second window threshold ensures the detector captures actively waiting vehicles while filtering out momentary
occupancy from free-flowing traffic.

##### Pedestrian and Bus Features

**Pedestrian Demand Indicator:**

$$
\phi_{ped}^{(i)} = \begin{cases}
1.0 & \text{if pedestrians waiting at intersection } i \\
0.0 & \text{otherwise}
\end{cases}
$$

Since there is no exclusive pedestrian phase in the DRL implementation, pedestrians cross during regular vehicle phases
(primarily P1 and P3 which serve through movements). The pedestrian demand indicator helps the agent learn to balance
pedestrian service with vehicle flow.

**Bus Presence Indicator:**

$$
\phi_{bus}^{(i)} = \begin{cases}
1.0 & \text{if bus detected on major arterial approach at intersection } i \\
0.0 & \text{otherwise}
\end{cases}
$$

Buses operate on a 15-minute headway along the major arterial. The agent learns to provide priority service to buses
through the reward function's modal equity weights.

##### Feature Normalization

All continuous features are normalized to $[0,1]$ to stabilize neural network training and ensure balanced gradient
magnitudes:

**Phase Duration Normalization:**

$$
d_{norm}^{(i)} = \min\left(\frac{d^{(i)}}{60}, 1.0\right)
$$

where $d^{(i)}$ is the current phase duration in seconds. The 60-second normalization factor corresponds to a typical
maximum phase duration.

**Binary Features:**

Queue occupancy, pedestrian demand, and bus presence indicators are already binary $\{0, 1\}$ and require no
normalization.

##### State Space Visualization

```mermaid
flowchart LR
    subgraph State["State Space Structure (32 dimensions)"]
        Int3["EACH INTERSECTION (16 dims)"]
    end

    subgraph Int3Features["State Features for Each Intersection"]
        Phase3["Phase Encoding<br>[1,0,0,0]<br>One-hot: 4 dims<br>(P1, P2, P3, P4)"]
        Duration3["Phase Duration<br>0.25 (normalized)<br>1 dim"]
        Queues3["Vehicle Queues<br>[0.8, 0.3, 0.0, 0.5]<br>4 approaches: 4 dims"]
        Bikes3["Bicycle Queues<br>[0.6, 0.2, 0.0, 0.3]<br>4 approaches: 4 dims"]
        Bus3["Bus Presence<br>0.0 (no bus)<br>Binary: 1 dim"]
        BusWait["Bus Normalized Wait<br>0.35<br>1 dim"]
        Time3["Simulation Time<br>0.42 (normalized)<br>1 dim"]
    end

    State --> Int3
    Int3 --> Phase3 --> Duration3 --> Queues3 --> Bikes3 --> Bus3 --> BusWait --> Time3

    style State fill:#E3F2FD
    style Int3 fill:#BBDEFB
    style Int3Features fill:#E8F5E9
    style Phase3 fill:#81C784
    style Duration3 fill:#AED581
    style Queues3 fill:#C5E1A5
    style Bikes3 fill:#DCEDC8
    style Bus3 fill:#FFCC80
    style BusWait fill:#FFB74D
    style Time3 fill:#FFA726
```

##### Key Differences from Rule-Based Control

This DRL implementation differs from the thesis's rule-based "Developed Control" in several important ways:

| Feature                   | Rule-Based (Thesis)                                  | DRL (This Work)                              |
| ------------------------- | ---------------------------------------------------- | -------------------------------------------- |
| **Control Architecture**  | Decentralized with semi-sync                         | Centralized single agent                     |
| **Phase Structure**       | 5 phases (P1-P5)                                     | 4 phases (P1-P4)                             |
| **Pedestrian Service**    | Exclusive Phase 5 (15s fixed)                        | Served during P1/P3                          |
| **Synchronization**       | 22s timer offset between intersections               | Both signals change simultaneously           |
| **Decision Logic**        | Hierarchical rules (max green, sync, bus, actuation) | Learned Q-value policy                       |
| **Bicycle Priority**      | Explicit detector window logic                       | Implicit through reward learning             |
| **Coordination Strategy** | Phase skipping for upstream-downstream coordination  | Natural coordination via centralized control |

The centralized approach is well-suited for closely-spaced intersections (300m) and serves as a baseline for future
multi-agent implementations where semi-synchronization strategies will be explored for longer arterial corridors.

##### Action Space

The action space $\mathcal{A}$ consists of three discrete control actions applied simultaneously to both intersections:

$$
\mathcal{A} = \{a_0, a_1, a_2\}
$$

Since the DRL agent uses centralized control, when an action is selected, it is executed at **both intersections
simultaneously**, maintaining perfect phase synchronization.

**Action Definitions:**

- **$a_0$ (Continue Current Phase):**

    - Maintains green signal on current movement
    - Phase duration counter increments by 1 second
    - Applied when traffic is clearing efficiently or minimum green time not yet satisfied
    - Most frequently selected action (~85% during training)

- **$a_1$ (Skip to Phase 1):**

    - Forces immediate transition to Phase 1 (major arterial through) from any other phase
    - Bypasses standard phase sequence when minor phases have cleared
    - Prioritizes major arterial traffic flow
    - Useful for quickly returning to Phase 1 after serving minor movements
    - Executed only if minimum green time constraint satisfied for current phase

- **$a_2$ (Progress to Next Phase):**
    - Advances through standard phase sequence: $1 \to 2 \to 3 \to 4 \to 1$
    - Provides balanced service across all movements
    - Executed only if minimum green time constraint satisfied for current phase
    - Typical selection rate ~12.5% during training

**Centralized Execution:**

When the agent selects action $a_t$ at timestep $t$:

$$
\begin{align}
\text{TLS}_3 &\leftarrow \text{execute}(a_t) \\
\text{TLS}_6 &\leftarrow \text{execute}(a_t)
\end{align}
$$

Both traffic signals receive the **same action** and transition to the **same phase** simultaneously, ensuring natural
coordination without requiring explicit synchronization timing mechanisms.

**Phase-Specific Safety Constraints:**

All phase-changing actions enforce minimum green time requirements that vary by phase:

$$
a \in \{a_1, a_2\} \implies d^{(i)} \geq d_{min}(p^{(i)})
$$

where the minimum green time depends on the current phase:

$$
d_{min}(p) = \begin{cases}
8 \text{ seconds} & \text{if } p = \text{Phase 1 (major through)} \\
3 \text{ seconds} & \text{if } p = \text{Phase 2 (major left)} \\
5 \text{ seconds} & \text{if } p = \text{Phase 3 (minor through)} \\
2 \text{ seconds} & \text{if } p = \text{Phase 4 (minor left)}
\end{cases}
$$

These minimum green times ensure sufficient clearance time for vehicles and bicycles that entered the intersection
during the green phase.

**Change Intervals:**

When a phase-changing action ($a_1$ or $a_2$) is executed, SUMO automatically inserts safety clearance intervals:

1. **Yellow interval**: 3 seconds (warning of phase change)
2. **All-red interval**: 2 seconds (clearance time for intersection)
3. **Leading green**: 1 second (priority start for vulnerable modes in new phase)

Total change interval duration: 6 seconds before the next main green phase begins.

**Maximum Green Time Enforcement:**

To prevent indefinite phase continuation, maximum green time limits are enforced:

$$
d^{(i)} \geq d_{max}(p^{(i)}) \implies \text{force phase change}
$$

where:

$$
d_{max}(p) = \begin{cases}
44 \text{ seconds} & \text{if } p = \text{Phase 1} \\
15 \text{ seconds} & \text{if } p = \text{Phase 2} \\
24 \text{ seconds} & \text{if } p = \text{Phase 3} \\
12 \text{ seconds} & \text{if } p = \text{Phase 4}
\end{cases}
$$

When maximum green time is reached, the agent's Continue action is overridden and the phase automatically advances to
the next phase in sequence.

---

---

# Multi-Objective Reward Function

The reward function $r_t = R(s_t, a_t, s_{t+1})$ balances competing objectives through weighted component summation,
evaluating action quality in the context of current and resulting traffic states. The function comprises **14 distinct
components** organized into three categories: **Environmental Feedback** (components 1-6, 8-13), **Meta-Level Guidance**
(component 7), and **Constraint Enforcement** (component 14).

##### Reward Philosophy and Design Principles

##### Two-Tiered Feedback Architecture

The reward function employs a dual-feedback mechanism distinguishing between environmental outcomes and training
statistics:

**1. Environmental Rewards (Components 1-6, 8-13)**

These measure **actual traffic consequences** resulting from control actions:

- Applied universally during both exploration (random actions) and exploitation (policy actions)
- Encode domain knowledge about traffic flow, safety, emissions, and modal equity
- Enable experience replay learning from diverse state-action-outcome trajectories

**Rationale:** Even random exploration teaches valuable lessons. A random "Skip to P1" from Phase 3 that reduces waiting
time by 15 seconds provides a genuine learning signal: "In states similar to X, skipping to P1 yields positive
outcomes."

**2. Meta-Level Rewards (Component 7)**

These measure **training statistics** to guide policy development:

- NOT applied during random exploration
- Only applied when agent deliberately selects actions ($\epsilon$-greedy chooses policy)
- Shape learning toward desired behavioral patterns (action diversity)

**Rationale:** Training statistics don't reflect environmental reality. If random exploration selects "Skip to P1" 900
times in 1000 steps, penalizing for "overuse" would incorrectly teach the agent that skipping is inherently bad, despite
never having chosen it intentionally.

```mermaid
flowchart LR
    A["üö¶ Traffic State<br>Observations"] --> B["ü§ñ DRL Agent<br>Selects Action"]

    B --> C["‚öôÔ∏è Execute Action<br>in Environment"]

    C --> D["üìä Measure<br>Outcomes"]

    D --> E1["‚öñÔ∏è Efficiency<br>‚Ä¢ Waiting times ‚Üì<br>‚Ä¢ Flow ‚Üë<br>‚Ä¢ Emissions ‚Üì"]
    D --> E2["üõ°Ô∏è Safety<br>‚Ä¢ No violations<br>‚Ä¢ Valid actions<br>‚Ä¢ Safe timing"]
    D --> E3["‚öñÔ∏è Equity<br>‚Ä¢ Modal fairness<br>‚Ä¢ Vulnerable users<br>‚Ä¢ Bus priority"]
    D --> E4["üéØ Strategy<br>‚Ä¢ Phase stability<br>‚Ä¢ Smart transitions<br>‚Ä¢ Coordination"]

    E1 --> F["‚ûï Reward<br>Aggregation"]
    E2 --> F
    E3 --> F
    E4 --> F

    F --> G["üìà Total Reward<br>r_t<br>(Clipped: -10 to +10)"]

    G --> H["üß† Learning<br>Update Policy"]

    H --> I["üìö Experience<br>Replay Buffer<br>(Prioritized)"]

    I --> J["üîÑ Improved<br>Decision Making"]

    J --> B

    style A fill:#E3F2FD
    style B fill:#C8E6C9
    style C fill:#FFF9C4
    style D fill:#FFCCBC

    style E1 fill:#81C784
    style E2 fill:#E57373
    style E3 fill:#64B5F6
    style E4 fill:#FFB74D

    style F fill:#CE93D8
    style G fill:#BA68C8
    style H fill:#AB47BC
    style I fill:#D1C4E9
    style J fill:#66BB6A
```

##### Complete Reward Formulation

$$
\begin{align}
r_t = \,& r_{wait} + r_{flow} + r_{CO_2} + r_{equity} + r_{safety} + r_{block} \\
& + r_{diversity} + r_{skip\_eff} + r_{skip\_inc} + r_{bus} \\
& + r_{next} + r_{stability} + r_{early} + r_{consec}
\end{align}
$$

Subject to stability clipping:

$$
r_t = \text{clip}(r_t, -10.0, +10.0)
$$

This clipping prevents extreme reward magnitudes from destabilizing neural network training while preserving relative
reward differences for effective learning.

```mermaid
flowchart TD
    A["üéØ Multi-Objective Reward Function<br>Balances 14 Components"] --> B["Environmental Feedback<br>(Actual Traffic Consequences)"]
    A --> C["Meta-Level Guidance<br>(Policy Shaping)"]
    A --> D["Constraint Enforcement<br>(Safety & Operations)"]

    B --> B1["Primary Objectives"]
    B --> B2["Critical Constraints"]
    B --> B3["Secondary Objectives"]
    B --> B4["Strategic Guidance"]

    B1 --> B1a["‚öñÔ∏è Waiting Time<br>Dominant penalty<br>Modal priority weighting<br>(Bus 2.0x, Car 1.3x, Bike/Ped 1.0x)"]
    B1 --> B1b["üöó Flow Bonus<br>Positive reinforcement<br>for vehicle movement"]

    B2 --> B2a["‚ö†Ô∏è Safety Violations<br>Large penalty<br>prevents unsafe policies"]
    B2 --> B2b["üö´ Blocked Actions<br>Discourages<br>infeasible decisions"]

    B3 --> B3a["üå± CO‚ÇÇ Emissions<br>Environmental<br>sustainability"]
    B3 --> B3b["‚öñÔ∏è Modal Equity<br>Fair service<br>distribution"]

    B4 --> B4a["üéØ Strategic Action Rewards<br>‚Ä¢ Skip to P1 effectiveness<br>‚Ä¢ Next phase bonus<br>‚Ä¢ Stability bonus"]
    B4 --> B4b["üöå Bus Priority<br>Public transit<br>assistance bonuses"]
    B4 --> B4c["‚è±Ô∏è Timing Penalties<br>‚Ä¢ Early phase change<br>‚Ä¢ Excessive continuation"]

    C --> C1["üé≤ Action Diversity<br>Prevents policy collapse<br>Encourages exploration<br>(Only during policy learning)"]

    D --> D1["üõ°Ô∏è Operational Safety<br>Min/max green times<br>Valid phase transitions"]

    B1a --> E["Total Reward Signal<br>r_t ‚àà [-10, +10]"]
    B1b --> E
    B2a --> E
    B2b --> E
    B3a --> E
    B3b --> E
    B4a --> E
    B4b --> E
    B4c --> E
    C1 --> E
    D1 --> E

    E --> F["üß† Neural Network<br>Learning"]
    F --> G["üìà Policy Improvement<br>Over Episodes"]
    G --> H["‚úÖ Optimized Control<br>‚Ä¢ Efficient traffic flow<br>‚Ä¢ Vulnerable user priority<br>‚Ä¢ Safe operations<br>‚Ä¢ Modal equity"]

    style A fill:#E3F2FD
    style B fill:#C8E6C9
    style C fill:#FFF9C4
    style D fill:#FFCCBC

    style B1 fill:#81C784
    style B2 fill:#E57373
    style B3 fill:#64B5F6
    style B4 fill:#FFB74D

    style B1a fill:#A5D6A7
    style B1b fill:#A5D6A7
    style B2a fill:#EF9A9A
    style B2b fill:#EF9A9A
    style B3a fill:#90CAF9
    style B3b fill:#90CAF9
    style B4a fill:#FFCC80
    style B4b fill:#FFCC80
    style B4c fill:#FFCC80

    style C1 fill:#FFF59D
    style D1 fill:#FFAB91

    style E fill:#CE93D8
    style F fill:#BA68C8
    style G fill:#AB47BC
    style H fill:#66BB6A
```

**Recommendation**: Use the **first diagram** (comprehensive breakdown) for your paper as it shows all 14 components
organized by category, which directly maps to your paper's structure. The hierarchical diagram could go in supplementary
materials or slides.

The key insight these diagrams convey is that your reward function isn't just a simple weighted sum - it's a **carefully
designed hierarchical system** where safety constraints override efficiency goals, efficiency goals dominate the
learning signal, and secondary objectives provide refinement. This qualitative view helps readers understand the
**design philosophy** rather than getting lost in the mathematical details.

##### Component 1: Weighted Waiting Time Penalty

**Type:** Environmental | **Priority:** Primary Objective

The dominant reward component penalizes stopped or slow-moving vehicles, weighted by modal transportation priorities:

$$
r_{wait} = r_{wait}^{base} + r_{wait}^{excessive}
$$

###### Base Waiting Penalty

$$
r_{wait}^{base} = -\alpha_{wait} \cdot \rho_{stopped}
$$

where $\alpha_{wait} = 2.5$ and the normalized stopped ratio is:

$$
\rho_{stopped} = \frac{\sum_{m \in M} n_{stopped}^{(m)} \cdot w_m}{\sum_{m \in M} n_{total}^{(m)} \cdot w_m}
$$

**Modal Priority Weights:**

$$
w_m = \begin{cases}
1.3 & m = \text{car} \\
1.0 & m = \text{bicycle} \\
1.0 & m = \text{pedestrian} \\
2.0 & m = \text{bus}
\end{cases}
$$

**Weight Rationale:**

- **Buses ($w = 2.0$)**: Highest priority incentivizes public transit efficiency; each bus serves 30-50 passengers,
  yielding substantial per-capita benefit
- **Cars ($w = 1.3$)**: Moderate priority reflects traffic capacity and throughput importance while discouraging
  single-occupancy vehicle dominance
- **Bicycles & Pedestrians ($w = 1.0$)**: Baseline priority ensures vulnerable road user protection without overwhelming
  optimization

**Stopped Vehicle Classification:**

A vehicle is classified as "stopped" when instantaneous speed falls below:

$$
v < v_{threshold} = 0.1 \text{ m/s}
$$

This threshold captures both completely stopped vehicles (queuing at red lights) and slow-crawling congestion
(stop-and-go traffic).

###### Excessive Waiting Penalty

To prevent pathologically long waiting times that severely degrade service quality, additional quadratic penalties apply
when modal average waiting times exceed acceptable thresholds:

$$
r_{wait}^{excessive} = r_{wait}^{car} + r_{wait}^{bike}
$$

**Car Excessive Waiting:**

$$
r_{wait}^{car} = \begin{cases}
-1.5 \cdot \frac{\bar{w}_{car} - 30}{30} & \text{if } 30 < \bar{w}_{car} \leq 40 \\[0.3em]
-1.5 \cdot \frac{\bar{w}_{car} - 30}{30} - 2.0 \cdot \left(\frac{\bar{w}_{car} - 40}{40}\right)^2 & \text{if } \bar{w}_{car} > 40 \\[0.3em]
0 & \text{otherwise}
\end{cases}
$$

**Bicycle Excessive Waiting:**

$$
r_{wait}^{bike} = \begin{cases}
-0.75 \cdot \frac{\bar{w}_{bike} - 25}{25} & \text{if } 25 < \bar{w}_{bike} \leq 35 \\[0.3em]
-0.75 \cdot \frac{\bar{w}_{bike} - 25}{25} - 2.0 \cdot \left(\frac{\bar{w}_{bike} - 35}{35}\right)^2 & \text{if } \bar{w}_{bike} > 35 \\[0.3em]
0 & \text{otherwise}
\end{cases}
$$

where $\bar{w}_m$ is the average waiting time (seconds) for mode $m$.

**Penalty Structure:**

- **Linear region** (30-40s cars, 25-35s bikes): Moderate penalty encourages service improvement
- **Quadratic region** (>40s cars, >35s bikes): Severe penalty prevents extreme delays that would cause driver/cyclist
  frustration and route abandonment

**Range:** $r_{wait} \in [-8.0, 0]$ (typical: $[-3.5, 0]$)

##### Component 2: Traffic Flow Bonus

**Type:** Environmental | **Priority:** Secondary Objective

Provides positive reinforcement for vehicle movement, creating dense reward signal:

$$
r_{flow} = (1.0 - \rho_{stopped}) \times 0.5
$$

where $\rho_{stopped}$ is the normalized stopped ratio from Component 1.

**Design Rationale:**

The asymmetric structure (waiting penalty magnitude $\alpha_{wait} = 2.5$ vs. flow bonus coefficient $0.5$) ensures the
agent prioritizes **reducing congestion** over marginally increasing flow. This prevents the agent from learning risky
policies that maximize throughput at the expense of safety or fairness.

**Interpretation:**

- $\rho_{stopped} = 0$ (all vehicles moving): $r_{flow} = +0.5$ (maximum bonus)
- $\rho_{stopped} = 0.5$ (half stopped): $r_{flow} = +0.25$ (moderate bonus)
- $\rho_{stopped} = 1.0$ (complete gridlock): $r_{flow} = 0$ (no bonus)

**Range:** $r_{flow} \in [0, 0.5]$

##### Component 3: CO‚ÇÇ Emissions Penalty

**Type:** Environmental | **Priority:** Environmental Sustainability

Penalizes carbon dioxide emissions from vehicular traffic:

$$
r_{CO_2} = -\alpha_{emission} \times \frac{\sum_{v \in V} e_v^{CO_2}}{|V| + 1}
$$

where $\alpha_{emission} = 0.05$ and $e_v^{CO_2}$ is the instantaneous CO‚ÇÇ emission rate (mg/s) for vehicle $v$.

**SUMO Emission Model:**

SUMO employs the HBEFA (Handbook Emission Factors for Road Transport) model, computing emissions from:

- Vehicle speed and acceleration profiles
- Vehicle type and engine characteristics (gasoline, diesel, electric)
- Driving conditions (idling, acceleration, cruising, deceleration)

**Peak Emission Scenarios:**

1. **Idling at red lights**: Engine running, zero progress ‚Üí high emissions per unit distance
2. **Stop-and-go traffic**: Repeated acceleration/deceleration cycles ‚Üí inefficient combustion
3. **Rapid acceleration**: High power demand ‚Üí fuel-rich mixture

**Normalization Strategy:**

Division by $(|V| + 1)$ normalizes emissions by vehicle count, ensuring episodes with different traffic volumes produce
comparable reward signals. The $+1$ term prevents division by zero in edge cases (no vehicles present). This would be
clipped within the total reward range $[-10, +10]$.

**Range:** $r_{CO_2} \in [-10.0, 0]$ (typical: $[-0.5, 0]$ for moderate traffic)

##### Component 4: Inter-Modal Equity Penalty

**Type:** Environmental | **Priority:** Fairness Objective

Penalizes unfair waiting time distribution across transportation modes:

$$
r_{equity} = -\alpha_{equity} \times CV_{wait}
$$

where $\alpha_{equity} = 0.5$ and the Coefficient of Variation measures relative disparity:

$$
CV_{wait} = \frac{\sigma(\bar{w}_m)}{\mu(\bar{w}_m) + \epsilon}
$$

with $\epsilon = 1.0$ preventing division by zero.

**Modal Average Waiting Times:**

For each mode $m \in \{\text{car, bicycle, pedestrian, bus}\}$:

$$
\bar{w}_m = \frac{1}{|V_m|} \sum_{v \in V_m} w_v
$$

**Statistical Measures:**

$$
\begin{align}
\mu(\bar{w}_m) &= \frac{1}{|M|} \sum_{m \in M} \bar{w}_m \quad \text{(mean across modes)} \\[0.3em]
\sigma(\bar{w}_m) &= \sqrt{\frac{1}{|M|} \sum_{m \in M} (\bar{w}_m - \mu(\bar{w}_m))^2} \quad \text{(standard deviation)}
\end{align}
$$

**Equity Interpretation:**

$$
CV_{wait} = \begin{cases}
\approx 0 & \text{Equitable: all modes experience similar waits} \\
0.3 - 0.7 & \text{Moderate disparity: some imbalance} \\
> 1.0 & \text{Severe inequity: one mode dominates service}
\end{cases}
$$

**Clamping:** $CV_{wait} = \min(CV_{wait}, 1.0)$ prevents extreme values from dominating the reward signal.

This penalty encourages balanced service: reducing bicycle wait from 45s to 30s (matching cars) would eliminate the
equity penalty.

**Range:** $r_{equity} \in [-0.5, 0]$

##### Component 5: Safety Violation Penalty

**Type:** Environmental | **Priority:** Critical Constraint

Enforces traffic safety through collision risk detection and penalty:

$$
r_{safety} = \begin{cases}
+0.05 & \text{if } n_{violations} = 0 \\[0.3em]
-\alpha_{safety} \times \min\left(\frac{n_{violations}}{\tau_{safety}}, 1.0\right) & \text{if } n_{violations} > 0
\end{cases}
$$

where $\alpha_{safety} = 2.0$, $\tau_{safety} = 3.0$, and:

$$
n_{violations} = n_{headway} + n_{distance}
$$

###### Safety Violation Types

**Type 1: Unsafe Headway**

Detects vehicles following too closely at high speed:

$$
\mathbb{1}_{headway} = \begin{cases}
1 & \text{if } h_{time} < 2.0 \text{ s AND } v_{follow} > 8.0 \text{ m/s} \\
0 & \text{otherwise}
\end{cases}
$$

where time headway is:

$$
h_{time} = \frac{d_{gap}}{v_{follow}}
$$

- $d_{gap}$: Gap distance between rear of lead vehicle and front of following vehicle (meters)
- $v_{follow}$: Speed of following vehicle (m/s)
- **2-second rule**: Standard safe following distance guideline
- **Speed threshold (8 m/s = 28.8 km/h)**: Only penalizes at speeds where collision would cause significant damage

**Type 2: Unsafe Distance**

Detects vehicles dangerously close while moving:

$$
\mathbb{1}_{distance} = \begin{cases}
1 & \text{if } d_{gap} < 5.0 \text{ m AND } v_{follow} > 1.0 \text{ m/s} \\
0 & \text{otherwise}
\end{cases}
$$

- **5m threshold**: Critical collision risk zone
- **Speed threshold (1 m/s)**: Distinguishes moving vehicles from stopped queue (normal at red lights)

**Design Rationale:**

The substantial penalty ($-2.0$) ensures safety violations significantly impact total reward, preventing the agent from
learning unsafe policies even if they marginally improve traffic flow. The small positive reward for violation-free
operation (+0.05) provides consistent reinforcement for safe control.

**Range:** $r_{safety} \in \{-2.0, +0.05\}$ (discrete values based on violation presence)

##### Component 6: Blocked Action Penalty

**Type:** Environmental | **Priority:** Execution Constraint

Penalizes attempted phase changes that violate minimum green time constraints:

$$
r_{block} = \begin{cases}
-\alpha_{block} & \text{if } a_t \in \{a_1, a_2\} \land d^{(i)} < d_{min}(p^{(i)}) \\[0.3em]
-\alpha_{block} \times 0.5 & \text{if } a_t = a_1 \land p^{(i)} = p_1 \text{ (already in P1)} \\[0.3em]
-\alpha_{block} \times 0.1 & \text{if blocked AND bus waiting} > 9\text{s} \\[0.3em]
0 & \text{otherwise}
\end{cases}
$$

where $\alpha_{block} = 0.1$.

**Minimum Green Time Constraints:**

$$
d_{min}(p) = \begin{cases}
8 \text{ s} & p = \text{Phase 1 (major through)} \\
3 \text{ s} & p = \text{Phase 2 (major left)} \\
5 \text{ s} & p = \text{Phase 3 (minor through)} \\
2 \text{ s} & p = \text{Phase 4 (minor left)}
\end{cases}
$$

**Penalty Variants:**

1. **Standard blocking ($-0.1$)**: Phase change attempted before minimum green satisfied
2. **Invalid action ($-0.05$)**: Skip to P1 attempted while already in Phase 1 (logically impossible)
3. **Bus-sensitive blocking ($-0.01$)**: Reduced penalty when blocking occurs with waiting buses (>9s), acknowledging
   the action had good intent despite premature timing

**Purpose:** Teaches the agent not to waste computational effort selecting infeasible actions. While the blocked action
is prevented by safety logic, the penalty encourages the agent to learn valid action timing.

**Range:** $r_{block} \in \{-0.1, -0.05, -0.01, 0\}$

##### Component 7: Action Diversity Incentive

**Type:** Meta-Level | **Priority:** Policy Shaping

**APPLICATION RULE:** Only applied when:

- Agent is in training mode and learning the optimal policy for traffic management
- Action was ONLY from exploitation from learn policy until this time in traning (hence, excludes random exploration)
- Sufficient experience accumulated and need at least 100 actions to be completed
- Not in high-exploration phase (epsilon ‚â§ 0.6)

Encourages balanced action distribution to prevent policy collapse:

$$
r_{diversity} = r_{diversity}^{continue} + r_{diversity}^{skip} + r_{diversity}^{next}
$$

**Expected Action Frequencies:**

$$
f_{expected} = \begin{cases}
0.85 & a = 0 \text{ (Continue)} \\
0.025 & a = 1 \text{ (Skip to P1)} \\
0.125 & a = 2 \text{ (Next)}
\end{cases}
$$

**Distribution Source Selection:**

$$
f_{actual}(a) = \begin{cases}
\frac{n_{Q}(a)}{\sum_i n_{Q}(i)} & \text{if } \sum_i n_{Q}(i) \geq 100 \text{ (use Q-value distribution)} \\[0.5em]
\frac{n_{train}(a)}{\sum_i n_{train}(i)} & \text{otherwise (use training action distribution)}
\end{cases}
$$

where $n_Q(a)$ counts how many times action $a$ had highest Q-value, and $n_{train}(a)$ counts actual training
selections.

**Diversity Scale:** $\eta = 1.0 - \epsilon$ (decreases with exploration rate)

##### Per-Action Diversity Rewards

**Continue (Action 0):**

$$
r_{diversity}^{continue} = \begin{cases}
+0.1 \times \frac{f_{expected}(0) - f_{actual}(0)}{f_{expected}(0)} \times \eta & \text{if } f_{actual}(0) < 0.8 \times f_{expected}(0) \\[0.3em]
0 & \text{otherwise}
\end{cases}
$$

Encourages Continue when underused (< 68% actual vs. 85% expected).

**Skip to P1 (Action 1):**

$$
r_{diversity}^{skip} = \begin{cases}
+0.5 \times \frac{f_{expected}(1) - f_{actual}(1)}{f_{expected}(1)} \times \eta & \text{if } f_{actual}(1) < f_{expected}(1) \\[0.3em]
-0.15 \times \frac{f_{actual}(1) - f_{expected}(1)}{f_{expected}(1)} \times \eta & \text{if } f_{actual}(1) > 3 \times f_{expected}(1)
\end{cases}
$$

Strong encouragement when underused (< 2.5%), moderate penalty when severely overused (> 7.5%).

**Next (Action 2):**

$$
r_{diversity}^{next} = \begin{cases}
-0.15 \times \frac{f_{actual}(2) - f_{expected}(2)}{f_{expected}(2)} \times \eta & \text{if } f_{actual}(2) > 1.5 \times f_{expected}(2) \\[0.3em]
0 & \text{otherwise}
\end{cases}
$$

Penalizes overuse (> 18.75% actual vs. 12.5% expected).

**Range:** $r_{diversity} \in [-0.2, +0.5]$ (highly variable based on policy behavior)

##### Component 8: Skip to P1 Effectiveness Bonus

**Type:** Environmental | **Priority:** Strategic Action Reward

Rewards effective use of Skip to P1 action based on current phase context:

$$
r_{skip\_eff} = \begin{cases}
\beta_{eff}(p^{(i)}) & \text{if } a_t = a_1 \land d^{(i)} \geq d_{min}(p^{(i)}) \land p^{(i)} \neq p_1 \\
0 & \text{otherwise}
\end{cases}
$$

**Phase-Specific Effectiveness Coefficients:**

$$
\beta_{eff}(p) = \begin{cases}
0.75 & p = p_2 \text{ (major protected left)} \\
0.90 & p = p_3 \text{ (minor through)} \\
0.60 & p = p_4 \text{ (minor protected left)} \\
0.0 & p = p_1 \text{ (already at target)}
\end{cases}
$$

**Effectiveness Rationale:**

- **From P3 (0.90)**: Most beneficial - minor arterial served, return to high-capacity major arterial maximizes
  throughput
- **From P2 (0.75)**: Reasonable - major left turns cleared, return to through traffic
- **From P4 (0.60)**: Moderate - minor left turns typically have lower demand, but may skip prematurely
- **From P1 (0.00)**: Illogical - already serving major arterial through traffic

This teaches: "Skipping from Phase 3 (or Phase 2 or 4) to Phase 1 is strategically valuable."

**Range:** $r_{skip\_eff} \in \{0, 0.60, 0.75, 0.90\}$

##### Component 9: Skip to P1 Incentive

**Type:** Environmental | **Priority:** Strategic Timing Guidance

Encourages or discourages actions based on phase duration optimality:

$$
r_{skip\_inc} = \begin{cases}
-0.12 & \text{if } a_t = a_0 \land p^{(i)} \in \{p_2, p_3, p_4\} \land d^{(i)} \geq d_{stab}(p^{(i)}) \\[0.3em]
+0.12 & \text{if } a_t = a_1 \land p^{(i)} \in \{p_2, p_3, p_4\} \land d^{(i)} \geq d_{stab}(p^{(i)}) \\[0.3em]
0 & \text{otherwise}
\end{cases}
$$

where $d_{stab}(p)$ is the stability threshold from Component 11.

**Incentive Logic:**

When in non-major phases (P2, P3, P4) and sufficient green time has been provided:

- **Continuing** is discouraged ($-0.12$): Minor phases have served their purpose
- **Skipping to P1** is encouraged ($+0.12$): Return to high-capacity major arterial

**Purpose:** Guides the agent toward efficient phase cycling - serve minor phases adequately but don't linger
unnecessarily.

**Range:** $r_{skip\_inc} \in \{-0.12, 0, +0.12\}$

##### Component 10: Bus Assistance Bonus

**Type:** Environmental | **Priority:** Public Transit Priority

Rewards or penalizes based on bus service quality:

$$
r_{bus} = r_{bus}^{penalty} + r_{bus}^{excellent} + r_{bus}^{skip}
$$

###### Bus Service Quality

**Excessive Wait Penalty:**

$$
r_{bus}^{penalty} = \begin{cases}
-0.2 \times \frac{\bar{w}_{bus} - 20}{20} & \text{if } \bar{w}_{bus} > 20 \text{ s} \\
0 & \text{otherwise}
\end{cases}
$$

Penalizes when buses wait longer than 20 seconds (unacceptable public transit delay).

**Excellent Service Bonus:**

$$
r_{bus}^{excellent} = \begin{cases}
+0.15 & \text{if } \bar{w}_{bus} < 5 \text{ s} \\
0 & \text{otherwise}
\end{cases}
$$

Rewards when buses experience minimal delay (< 5s), indicating green wave success.

**Skip Assistance Bonus:**

When Skip to P1 action executed and buses are waiting:

$$
r_{bus}^{skip} = \begin{cases}
+0.3 & \text{if } a_t = a_1 \land \bar{w}_{bus} > 10 \text{ s} \\
+0.2 & \text{if } a_t = a_1 \land 5 < \bar{w}_{bus} \leq 10 \text{ s} \\
+0.1 & \text{if } a_t = a_1 \land \bar{w}_{bus} \leq 5 \text{ s} \\
0 & \text{otherwise}
\end{cases}
$$

Rewards using Skip to P1 to assist waiting buses (Phase 1 serves major arterial where buses travel). The substantial
bonus teaches: "When buses wait moderately, skipping to P1 provides valuable assistance."

**Range:** $r_{bus} \in [-2.0, +0.45]$ (typical: $[-0.3, +0.3]$)

##### Component 11: Next Phase Bonus

**Type:** Environmental | **Priority:** Multi-Phase Service Encouragement

Rewards advancing to next phase after sufficient green time, promoting balanced multi-modal service:

$$
r_{next} = \begin{cases}
\alpha_{next} \times (1.0 + \rho_{optimal}) & \text{if conditions met} \\
0 & \text{otherwise}
\end{cases}
$$

where $\alpha_{next} = 2.0$ and:

$$
\rho_{optimal} = \min\left(\frac{d^{(i)}}{d_{max}(p^{(i)}) \times 0.5}, 1.0\right)
$$

**Conditions for Bonus:**

1. $a_t = a_2$ (Next action selected)
2. $d^{(i)} \geq d_{next}(p^{(i)})$ (minimum threshold met)
3. $d^{(i)} < d_{consec}(p^{(i)})$ (not excessively long)

**Next Phase Thresholds:**

$$
d_{next}(p) = \begin{cases}
12 \text{ s} & p = p_1 \\
5 \text{ s} & p = p_2 \\
7 \text{ s} & p = p_3 \\
4 \text{ s} & p = p_4
\end{cases}
$$

**Optimal Duration Bonus:**

The multiplier $(1.0 + \rho_{optimal})$ provides additional reward when phase duration approaches half the maximum green
time:

- At minimum threshold ($d = d_{next}$): $\rho_{optimal} \approx 0.27$, bonus = $2.0 \times 1.27 = 2.54$
- At optimal duration ($d = 0.5 \times d_{max}$): $\rho_{optimal} = 1.0$, bonus = $2.0 \times 2.0 = 4.0$
- Beyond optimal: bonus remains at $4.0$ (capped)

This substantial reward teaches: "Voluntarily advancing after adequate service time yields positive outcomes."

**Range:** $r_{next} \in \{0, 2.5, 4.0\}$ (varies by timing optimality)

##### Component 12: Stability Bonus

**Type:** Environmental | **Priority:** Phase Continuity Reward

Rewards maintaining stable green phases beyond minimum thresholds:

$$
r_{stability} = \begin{cases}
\alpha_{stab} \times (1.0 + \rho_{duration}) & \text{if conditions met} \\
0 & \text{otherwise}
\end{cases}
$$

where $\alpha_{stab} = 0.12$ and:

$$
\rho_{duration} = \frac{d^{(i)}}{d_{max}(p^{(i)})}
$$

**Conditions for Bonus:**

1. $a_t = a_0$ (Continue action selected)
2. $d^{(i)} \geq d_{stab}(p^{(i)})$ (stability threshold met)
3. $d^{(i)} < d_{consec}(p^{(i)})$ (not excessively long)

**Stability Thresholds:**

$$
d_{stab}(p) = \begin{cases}
10 \text{ s} & p = p_1 \\
4 \text{ s} & p = p_2 \\
6 \text{ s} & p = p_3 \\
3 \text{ s} & p = p_4
\end{cases}
$$

**Duration Multiplier:**

The factor $(1.0 + \rho_{duration})$ increases reward as phase duration approaches maximum:

- At stability threshold: Small bonus (~$0.12 \times 1.23 = 0.15$)
- Mid-duration: Moderate bonus (~$0.12 \times 1.5 = 0.18$)
- Near maximum: Larger bonus (~$0.12 \times 1.9 = 0.23$)

**Rationale:**

Prevents "phase thrashing" where the agent changes phases too frequently, disrupting platoon flow. Stable green phases
allow vehicle platoons to clear efficiently, reducing stop-and-go traffic patterns.

**Range:** $r_{stability} \in [0, 0.24]$ (varies by phase and duration)

##### Component 13: Early Phase Change Penalty

**Type:** Environmental | **Priority:** Premature Action Discouragement

Penalizes phase changes that occur before optimal service time:

$$
r_{early} = \begin{cases}
-0.5 \times \rho_{shortfall} & \text{if } a_t \neq a_0 \land d^{(i)} < d_{next}(p^{(i)}) \\
0 & \text{otherwise}
\end{cases}
$$

where the shortfall ratio is:

$$
\rho_{shortfall} = 1.0 - \frac{d^{(i)}}{d_{next}(p^{(i)})}
$$

**Penalty Severity Examples:**

- Change at 50% of optimal duration: $\rho_{shortfall} = 0.5$, penalty = $-0.25$
- Change at 25% of optimal duration: $\rho_{shortfall} = 0.75$, penalty = $-0.375$
- Change at optimal duration: $\rho_{shortfall} = 0$, penalty = $0$

**Purpose:**

Discourages premature phase changes that don't allow sufficient time for traffic clearing. Works in concert with
Component 11 (Next Bonus) to create optimal timing window:

- Too early: Early change penalty
- Optimal window: Next bonus reward
- Too late: Consecutive continue penalty (Component 14)

**Range:** $r_{early} \in [-0.5, 0]$

##### Component 14: Consecutive Continue Penalty

**Type:** Environmental | **Priority:** Phase Stagnation Prevention

Penalizes excessive phase continuation beyond optimal duration:

$$
r_{consec} = \begin{cases}
-(n_{streak} - (d_{consec}(p) - 1)) \times 0.01 & \text{if } a_t = a_0 \land n_{streak} \geq d_{consec}(p) \\
0 & \text{otherwise}
\end{cases}
$$

where $n_{streak}$ is the consecutive Continue action count for current phase.

**Consecutive Continue Thresholds:**

$$
d_{consec}(p) = \begin{cases}
30 \text{ s} & p = p_1 \\
10 \text{ s} & p = p_2 \\
15 \text{ s} & p = p_3 \\
8 \text{ s} & p = p_4
\end{cases}
$$

**Penalty Progression:**

At threshold: $-(1) \times 0.01 = -0.01$ 5 seconds over: $-(5) \times 0.01 = -0.05$ 10 seconds over:
$-(10) \times 0.01 = -0.10$

The penalty grows linearly with excessive continuation, eventually overwhelming any stability bonus.

**Streak Reset:**

When any action other than Continue is selected, the streak counter resets to 0 for all intersections.

**Purpose:**

Prevents the agent from getting "stuck" in a single phase, ensuring all traffic movements receive service. Particularly
important for minor phases where the agent might otherwise neglect low-volume approaches.

**Range:** $r_{consec} \in [-0.50, 0]$ (typical: $[-0.10, 0]$)

```mermaid
flowchart TD
    A["üéØ Total Reward Function"] --> B["Tier 1: Critical<br>Safety & Constraints<br>(Weight: HIGH)"]
    A --> C["Tier 2: Primary<br>Efficiency Goals<br>(Weight: DOMINANT)"]
    A --> D["Tier 3: Strategic<br>Behavioral Shaping<br>(Weight: MODERATE)"]
    A --> E["Tier 4: Secondary<br>Long-term Objectives<br>(Weight: LOW)"]

    B --> B1["‚ö†Ô∏è Safety Violations<br>Prevents catastrophic failures"]
    B --> B2["üö´ Blocked Actions<br>Ensures feasible control"]

    C --> C1["‚öñÔ∏è Waiting Time Penalty<br>PRIMARY objective<br>Modal priority weights"]
    C --> C2["üöó Traffic Flow Bonus<br>Positive reinforcement"]

    D --> D1["üéØ Strategic Rewards<br>‚Ä¢ Next phase bonus<br>‚Ä¢ Skip effectiveness<br>‚Ä¢ Stability rewards"]
    D --> D2["üöå Bus Priority<br>Public transit focus"]
    D --> D3["‚è±Ô∏è Timing Control<br>‚Ä¢ Early/late penalties<br>‚Ä¢ Phase duration"]

    E --> E1["üå± Emissions<br>Environmental impact"]
    E --> E2["‚öñÔ∏è Modal Equity<br>Service fairness"]
    E --> E3["üé≤ Diversity<br>(Training only)"]

    B1 --> F["‚ö° Immediate<br>Strong Penalties<br>Override other objectives"]
    B2 --> F

    C1 --> G["üí™ Dominant<br>Gradient Signal<br>Primary learning driver"]
    C2 --> G

    D1 --> H["üé® Behavioral<br>Shaping<br>Guides strategy"]
    D2 --> H
    D3 --> H

    E1 --> I["üåü Refinement<br>Tie-breakers<br>Secondary optimization"]
    E2 --> I
    E3 --> I

    F --> J["üßÆ Weighted<br>Aggregation"]
    G --> J
    H --> J
    I --> J

    J --> K["üìä Total Reward<br>Multi-objective balance<br>Clipped to [-10, +10]"]

    style A fill:#E3F2FD

    style B fill:#FFCDD2
    style B1 fill:#EF5350
    style B2 fill:#E57373
    style F fill:#D32F2F

    style C fill:#C8E6C9
    style C1 fill:#66BB6A
    style C2 fill:#81C784
    style G fill:#4CAF50

    style D fill:#FFE0B2
    style D1 fill:#FFB74D
    style D2 fill:#FFA726
    style D3 fill:#FF9800
    style H fill:#F57C00

    style E fill:#E1F5FE
    style E1 fill:#64B5F6
    style E2 fill:#42A5F5
    style E3 fill:#2196F3
    style I fill:#1976D2

    style J fill:#CE93D8
    style K fill:#9C27B0
```

##### Reward Component Summary Table

| Component                    | Symbol          | Type          | Weight                   | Range                       | Application         |
| ---------------------------- | --------------- | ------------- | ------------------------ | --------------------------- | ------------------- |
| **1. Waiting Time**          | $r_{wait}$      | Environmental | $\alpha=2.5$ + Excessive | $[-8.0, 0]$                 | Universal           |
| **2. Flow Bonus**            | $r_{flow}$      | Environmental | $0.5$                    | $[0, 0.5]$                  | Universal           |
| **3. CO‚ÇÇ Emissions**         | $r_{CO_2}$      | Environmental | $\alpha=0.05$            | $[-10.0, 0]$                | Universal           |
| **4. Equity**                | $r_{equity}$    | Environmental | $\alpha=0.5$             | $[-0.5, 0]$                 | Universal           |
| **5. Safety**                | $r_{safety}$    | Environmental | $\alpha=2.0$             | $\{-2.0, +0.05\}$           | Universal           |
| **6. Blocked Action**        | $r_{block}$     | Environmental | $\alpha=0.1$             | $\{-0.1, -0.05, -0.01, 0\}$ | Universal           |
| **7. Diversity**             | $r_{diversity}$ | Meta-Level    | Variable                 | $[-0.2, +0.5]$              | Policy actions only |
| **8. Skip Effectiveness**    | $r_{skip\_eff}$ | Environmental | $0.60-0.90$              | $\{0, 0.60, 0.75, 0.90\}$   | Universal           |
| **9. Skip Incentive**        | $r_{skip\_inc}$ | Environmental | $0.12$                   | $\{-0.12, 0, +0.12\}$       | Universal           |
| **10. Bus Assistance**       | $r_{bus}$       | Environmental | Variable                 | $[-2.0, +0.45]$             | Universal           |
| **11. Next Bonus**           | $r_{next}$      | Environmental | $\alpha=2.0$             | $\{0, 2.5, 4.0\}$           | Universal           |
| **12. Stability**            | $r_{stability}$ | Environmental | $\alpha=0.12$            | $[0, 0.24]$                 | Universal           |
| **13. Early Change**         | $r_{early}$     | Environmental | $0.5$                    | $[-0.5, 0]$                 | Universal           |
| **14. Consecutive Continue** | $r_{consec}$    | Environmental | $0.01/\text{s}$          | $[-0.50, 0]$                | Universal           |

##### Design Principles and Justification

###### 1. **Hierarchical Objective Structure**

**Primary Objective (Traffic Efficiency):**

- Waiting time penalty ($\alpha = 2.5$): Dominates reward signal
- Flow bonus (coefficient $0.5$): Provides positive reinforcement

**Critical Constraints (Safety):**

- Safety penalty ($\alpha = 2.0$): Prevents catastrophic failures
- High magnitude ensures safe policies even at cost of marginal efficiency

**Secondary Objectives (Sustainability & Equity):**

- Emissions ($\alpha = 0.05$): Modest influence, breaks efficiency ties
- Equity ($\alpha = 0.5$): Moderate influence, ensures fairness

**Behavioral Shaping (Strategic Actions):**

- Next bonus ($\alpha = 2.0$): Strong encouragement for balanced service
- Stability ($\alpha = 0.12$): Subtle guidance toward smooth operation
- Skip effectiveness ($\beta = 0.60-0.90$): Strategic action value signals

###### 2. **Multi-Scale Temporal Rewards**

- **Instantaneous** (safety, flow): Immediate feedback on current state
- **Short-term** (stability, early change): Rewards/penalties over 5-15 seconds
- **Medium-term** (waiting time, emissions): Aggregated over 30-60 seconds
- **Long-term** (equity, diversity): Episode-level statistics

###### 3. **Asymmetric Penalties vs. Bonuses**

Penalties generally exceed bonuses in magnitude (e.g., $r_{wait} \in [-8, 0]$ vs. $r_{flow} \in [0, 0.5]$), creating
risk-averse policies that prioritize avoiding bad outcomes over pursuing marginal gains.

###### 4. **Phase-Aware Reward Structure**

Many components use phase-specific parameters:

- Minimum green times: $\{8, 3, 5, 2\}$ seconds
- Skip effectiveness: $\{0.75, 0.90, 0.60\}$ bonuses
- Stability thresholds: $\{10, 4, 6, 3\}$ seconds

This encodes domain knowledge: major arterial phases (P1) require longer service than minor phases (P2, P4).

###### 5. **Dense Reward Signal**

All 14 components provide feedback at every timestep, creating dense gradient information for neural network learning.
This contrasts with sparse reward designs (e.g., only rewarding episode-end outcomes) that suffer from credit assignment
problems.

###### 6. **Modal Priority Encoding**

Waiting time weights ($w_{bus} = 2.0 > w_{car} = 1.3 > w_{bike} = w_{ped} = 1.0$) encode transportation policy
priorities while separate equity component ensures no mode is systematically neglected.

This comprehensive reward function provides **dense, multi-scale feedback** that guides the DRL agent toward **safe,
efficient, equitable, and environmentally sustainable** traffic signal control while maintaining **strategic
flexibility** through action-specific bonuses and phase-aware parameters. The 14-component structure balances competing
objectives through careful weight tuning informed by transportation engineering domain knowledge.

---

---

# Comprehensive Performance Analysis: Three-Way Comparison

##### Private Car Waiting Times

###### Performance Summary by Scenario Category

**Pr Scenarios (Varying Car Volume: 100-1000 veh/hr):**

| Metric                | Reference | Developed | DRL   | DRL vs Ref | DRL vs Developed |
| --------------------- | --------- | --------- | ----- | ---------- | ---------------- |
| Pr_0-3 Avg (Low-Mid)  | 21.8s     | 32.3s     | 28.4s | +30.3%     | -12.1%           |
| Pr_4-6 Avg (Mid-High) | 23.0s     | 37.7s     | 50.3s | +118.7%    | +33.4%           |
| Pr_7-9 Avg (High)     | 26.0s     | 44.0s     | 45.7s | +75.8%     | +3.9%            |

**Key Observations:**

- **Low-mid demand (Pr_0-3):** DRL performs between Reference and Developed, closer to Reference baseline
- **Mid-high demand (Pr_4-6):** DRL shows significant degradation (+118.7% vs Reference, +33.4% vs Developed)
- **High demand (Pr_7-9):** DRL converges with Developed control, both showing similar degradation
- **Pattern:** DRL struggles most at mid-high car volumes (500-700 veh/hr), suggesting agent prioritizes other modes
  during peak mixed demand

**Bi Scenarios (Constant 400 cars/hr, Varying Bicycle Volume):**

| Metric     | Reference | Developed | DRL   | DRL vs Ref | DRL vs Developed |
| ---------- | --------- | --------- | ----- | ---------- | ---------------- |
| Bi_0-3 Avg | 21.8s     | 33.5s     | 43.9s | +101.4%    | +31.0%           |
| Bi_4-6 Avg | 21.3s     | 38.0s     | 44.9s | +110.8%    | +18.2%           |
| Bi_7-9 Avg | 22.0s     | 41.3s     | 44.5s | +102.3%    | +7.8%            |

**Key Observations:**

- **Consistent degradation:** DRL adds ~10-12s to Developed control across all bicycle volumes
- **Gap narrows with bike volume:** DRL vs Developed gap shrinks from +31.0% to +7.8% as bikes increase
- **Trade-off visible:** As agent prioritizes bicycles (see Section 2), car waiting time penalty increases

**Pe Scenarios (Constant 400 cars/hr, Varying Pedestrian Volume):**

| Metric     | Reference | Developed | DRL   | DRL vs Ref | DRL vs Developed |
| ---------- | --------- | --------- | ----- | ---------- | ---------------- |
| Pe_0-3 Avg | 21.8s     | 32.0s     | 39.7s | +82.1%     | +24.1%           |
| Pe_4-6 Avg | 21.7s     | 36.3s     | 48.5s | +123.5%    | +33.6%           |
| Pe_7-9 Avg | 21.3s     | 37.7s     | 44.8s | +110.3%    | +18.8%           |

**Key Observations:**

- **Mid-range penalty:** Pe_4-6 shows highest car degradation (+123.5% vs Reference)
- **Agent behavior:** DRL agent significantly activates pedestrian phases during mid-high ped demand
- **Convergence at extremes:** At very high ped volumes (Pe_7-9), DRL performance stabilizes

###### Overall Car Waiting Time Assessment

**Averages Across All 30 Scenarios:**

- Reference: 22.3s
- Developed: 36.5s
- DRL: 43.8s

**Performance Gap:**

- DRL vs Reference: **+96.4%** (nearly double)
- DRL vs Developed: **+20.0%** (20% worse)

**Conclusion:** DRL agent consistently prioritizes vulnerable road users (bicycles, pedestrians, buses) at the expense
of private car throughput, resulting in 20% higher car waiting times compared to the already-degraded Developed control.

##### Bicycle Waiting Times

###### Performance Summary by Scenario Category

**Pr Scenarios (Constant 400 bikes/hr, Varying Car Volume):**

| Metric     | Reference | Developed | DRL   | DRL vs Ref | DRL vs Developed |
| ---------- | --------- | --------- | ----- | ---------- | ---------------- |
| Pr_0-3 Avg | 263.5s    | 31.8s     | 17.5s | **-93.4%** | **-45.0%**       |
| Pr_4-6 Avg | 55.3s     | 32.7s     | 22.2s | **-59.9%** | **-32.1%**       |
| Pr_7-9 Avg | 41.7s     | 36.0s     | 15.2s | **-63.5%** | **-57.8%**       |

**Key Observations:**

- **Dramatic improvements:** DRL achieves 45-58% better bicycle service than Developed control
- **Reference catastrophic:** Reference shows 41-264s waits, demonstrating car-centric design failure
- **Best at extremes:** DRL performs exceptionally well at low cars (Pr_0-3: 17.5s avg) and high cars (Pr_7-9: 15.2s
  avg)
- **Consistency:** DRL maintains sub-25s bicycle waits across all car volumes

**Bi Scenarios (Varying Bicycle Volume: 100-1000 bikes/hr):**

| Scenario Group    | Reference | Developed | DRL   | DRL vs Ref | DRL vs Developed |
| ----------------- | --------- | --------- | ----- | ---------- | ---------------- |
| Bi_0-3 (Low)      | 66.3s     | 30.3s     | 13.9s | **-79.0%** | **-54.1%**       |
| Bi_4-6 (Mid-High) | 381.0s    | 43.3s     | 34.7s | **-90.9%** | **-19.9%**       |
| Bi_7-9 (High)     | 637.3s    | 131.0s    | 42.4s | **-93.3%** | **-67.6%**       |

**Key Observations:**

- **Catastrophic Reference failure:** At high bike volumes (Bi_7-9), Reference control results in 10+ minute waits
- **Developed control breakdown:** Even Developed shows 131s average at Bi_7-9 (>2 minutes)
- **DRL excellence:** Maintains 42s average even at 1000 bikes/hr (Bi_9: 42.4s)
- **67.6% improvement over Developed:** At critical high-bike scenarios where congestion is most severe

**Pe Scenarios (Constant 400 bikes/hr, Varying Pedestrian Volume):**

| Metric     | Reference | Developed | DRL   | DRL vs Ref | DRL vs Developed |
| ---------- | --------- | --------- | ----- | ---------- | ---------------- |
| Pe_0-3 Avg | 133.5s    | 29.3s     | 20.2s | **-84.9%** | **-31.1%**       |
| Pe_4-6 Avg | 102.7s    | 33.7s     | 23.2s | **-77.4%** | **-31.2%**       |
| Pe_7-9 Avg | 101.7s    | 34.7s     | 21.0s | **-79.4%** | **-39.5%**       |

**Key Observations:**

- **Consistent 30-40% improvement:** DRL outperforms Developed by 31-40% across all ped volumes
- **Sub-25s guarantee:** DRL maintains excellent bike service (<25s) regardless of pedestrian demand
- **Multimodal balance:** Agent successfully serves both bicycles and pedestrians simultaneously

###### Overall Bicycle Waiting Time Assessment

**Averages Across All 30 Scenarios:**

- Reference: 208.5s (3.5 minutes)
- Developed: 48.1s
- DRL: 23.8s

**Performance Gap:**

- DRL vs Reference: **-88.6%** (11-second reduction per 10-second Reference baseline)
- DRL vs Developed: **-50.5%** (cuts waiting time in HALF)

**Conclusion:** DRL agent achieves **transformational improvement** for bicycle traffic, cutting Developed control
waiting times by 50.5% and reducing Reference control's unacceptable 3.5-minute average to a mere 24 seconds. This
represents the single strongest performance gain across all modes.

##### Pedestrian Waiting Times

###### Performance Summary by Scenario Category

**Pr Scenarios (Constant 400 peds/hr, Varying Car Volume):**

| Metric     | Reference | Developed | DRL  | DRL vs Ref | DRL vs Developed |
| ---------- | --------- | --------- | ---- | ---------- | ---------------- |
| Pr_0-3 Avg | 59.3s     | 11.3s     | 4.2s | **-92.9%** | **-62.8%**       |
| Pr_4-6 Avg | 12.7s     | 12.3s     | 3.3s | **-74.0%** | **-73.2%**       |
| Pr_7-9 Avg | 12.0s     | 13.7s     | 1.3s | **-89.2%** | **-90.5%**       |

**Key Observations:**

- **Exceptional performance:** DRL achieves sub-5s pedestrian waits across all car volumes
- **Best at high car volume:** Pr_7-9 shows remarkable 1.3s average (90.5% better than Developed)
- **62-90% improvement range:** Consistently outperforms Developed control by 63-91%
- **Reference baseline failure:** Pr_0 shows 129s (>2 minutes), demonstrating car-first design

**Bi Scenarios (Constant 400 peds/hr, Varying Bicycle Volume):**

| Metric     | Reference | Developed | DRL  | DRL vs Ref | DRL vs Developed |
| ---------- | --------- | --------- | ---- | ---------- | ---------------- |
| Bi_0-3 Avg | 13.3s     | 10.8s     | 2.9s | **-78.2%** | **-73.1%**       |
| Bi_4-6 Avg | 14.3s     | 12.7s     | 3.2s | **-77.6%** | **-74.8%**       |
| Bi_7-9 Avg | 16.3s     | 15.7s     | 2.2s | **-86.5%** | **-86.0%**       |

**Key Observations:**

- **Consistency across bike volumes:** DRL maintains 2-3s waits regardless of bicycle demand
- **73-86% improvement:** Developed control shows 11-16s waits; DRL reduces to 2-3s
- **Multimodal optimization:** Agent successfully prioritizes both bikes and pedestrians simultaneously

**Pe Scenarios (Varying Pedestrian Volume: 100-1000 peds/hr):**

| Scenario Group    | Reference | Developed | DRL  | DRL vs Ref | DRL vs Developed |
| ----------------- | --------- | --------- | ---- | ---------- | ---------------- |
| Pe_0-3 (Low)      | 10.8s     | 10.5s     | 2.1s | **-80.6%** | **-80.0%**       |
| Pe_4-6 (Mid-High) | 76.3s     | 17.3s     | 3.2s | **-95.8%** | **-81.5%**       |
| Pe_7-9 (High)     | 114.0s    | 37.3s     | 4.0s | **-96.5%** | **-89.3%**       |

**Key Observations:**

- **Critical high-demand performance:** At Pe_7-9 (800-1000 peds/hr), DRL achieves 4.0s vs Developed's 37.3s
- **89% improvement at extremes:** Where Developed control struggles most (high ped volume), DRL excels
- **Reference control collapse:** Pe_9 shows 126s (>2 minutes) in Reference; DRL maintains 4s
- **Scalability demonstrated:** DRL maintains excellent service even at 10x pedestrian demand increase

###### Overall Pedestrian Waiting Time Assessment

**Averages Across All 30 Scenarios:**

- Reference: 48.4s
- Developed: 17.0s
- DRL: 3.1s

**Performance Gap:**

- DRL vs Reference: **-93.6%** (reduces to 1/16th of Reference time)
- DRL vs Developed: **-81.8%** (reduces to less than 1/5th)

**Conclusion:** DRL agent delivers **outstanding pedestrian service**, achieving the highest percentage improvement
across all modes (81.8% better than Developed). The 3.1-second average represents near-immediate service for
pedestrians, compared to Developed's 17-second average and Reference's unacceptable 48-second waits.

##### Bus Waiting Times

###### Performance Summary by Scenario Category

**Pr Scenarios (Varying Car Volume):**

| Metric     | Reference | Developed | DRL   | DRL vs Ref | DRL vs Developed |
| ---------- | --------- | --------- | ----- | ---------- | ---------------- |
| Pr_0-3 Avg | 26.5s     | 11.0s     | 1.97s | **-92.6%** | **-82.1%**       |
| Pr_4-6 Avg | 25.0s     | 18.7s     | 11.1s | **-55.6%** | **-40.6%**       |
| Pr_7-9 Avg | 21.7s     | 26.7s     | 13.0s | **-40.1%** | **-51.3%**       |

**Key Observations:**

- **Excellent low-demand performance:** Pr_0-3 shows sub-2s bus waits (82% better than Developed)
- **Mid-high challenge:** Pr_4-6 shows degradation to 11s average (still 41% better than Developed)
- **Pattern:** Bus priority weakens as overall traffic increases, but still outperforms both baselines

**Bi Scenarios (Varying Bicycle Volume):**

| Metric     | Reference | Developed | DRL   | DRL vs Ref | DRL vs Developed |
| ---------- | --------- | --------- | ----- | ---------- | ---------------- |
| Bi_0-3 Avg | 24.0s     | 15.0s     | 2.30s | **-90.4%** | **-84.7%**       |
| Bi_4-6 Avg | 27.3s     | 14.7s     | 2.57s | **-90.6%** | **-82.5%**       |
| Bi_7-9 Avg | 21.7s     | 16.0s     | 2.21s | **-89.8%** | **-86.2%**       |

**Key Observations:**

- **Consistent excellence:** 2-3s average across all bicycle volumes
- **84-86% improvement:** Developed control shows 15-16s; DRL reduces to ~2s
- **Best category performance:** Bi scenarios show strongest bus priority

**Pe Scenarios (Varying Pedestrian Volume):**

| Metric     | Reference | Developed | DRL   | DRL vs Ref | DRL vs Developed |
| ---------- | --------- | --------- | ----- | ---------- | ---------------- |
| Pe_0-3 Avg | 24.8s     | 9.5s      | 2.62s | **-89.4%** | **-72.4%**       |
| Pe_4-6 Avg | 26.7s     | 17.3s     | 1.83s | **-93.1%** | **-89.4%**       |
| Pe_7-9 Avg | 26.7s     | 17.7s     | 3.61s | **-86.5%** | **-79.6%**       |

**Key Observations:**

- **Mid-range excellence:** Pe_4-6 shows best performance (1.83s, 89% improvement)
- **Consistent sub-4s service:** Even at high ped volumes, buses average 3.6s
- **72-89% improvement range:** Substantial gains across all pedestrian demand levels

###### Overall Bus Waiting Time Assessment

**Averages Across All 30 Scenarios:**

- Reference: 25.2s
- Developed: 16.2s
- DRL: 4.2s

**Performance Gap:**

- DRL vs Reference: **-83.3%** (reduces to 1/6th of Reference time)
- DRL vs Developed: **-74.1%** (reduces to 1/4th of Developed time)

**Conclusion:** DRL agent provides **excellent bus priority service**, achieving 74.1% improvement over Developed
control. The 4.2-second average demonstrates effective transit priority, ensuring buses experience minimal delays across
diverse traffic conditions.

##### Cross-Mode Performance Synthesis

###### Trade-off Analysis

**DRL Policy Characteristics:**

| Mode            | Performance vs Developed | Strategic Priority   | Service Quality      |
| --------------- | ------------------------ | -------------------- | -------------------- |
| **Cars**        | -20.0% (worse)           | **Medium priority**  | Acceptable (44s avg) |
| **Bicycles**    | +50.5% (better)          | **High priority**    | Excellent (24s avg)  |
| **Pedestrians** | +81.8% (better)          | **Highest priority** | Outstanding (3s avg) |
| **Buses**       | +74.1% (better)          | **High priority**    | Excellent (4s avg)   |

**The Multimodal Equity Trade-off:**

- **Sacrifice:** +7.3s average car delay (36.5s ‚Üí 43.8s) vs Developed
- **Gain:** -24.3s bicycle, -13.9s pedestrian, -12.0s bus improvements

**Person-Throughput Optimization:**

- Cars (avg 1.5 occupancy): -20% efficiency
- Buses (avg 30 occupancy): +74% efficiency
- Net Result: **Significant person-throughput gain** despite car degradation

###### Scenario-Specific Insights

**Pr Scenarios (Car-Focused):**

- Agent recognizes increasing car demand and adapts
- Still prioritizes vulnerable users, but with less aggressive trade-offs
- Pr_7-9: DRL converges with Developed for cars, excels for others

**Bi Scenarios (Bicycle-Focused):**

- Agent's strongest performance domain
- Bi_9 (1000 bikes/hr): Maintains 42s vs Developed's 205s (79% better)
- Demonstrates scalability to extreme bicycle volumes

**Pe Scenarios (Pedestrian-Focused):**

- Critical test of high-demand handling
- Pe_7-9: DRL maintains 3-5s ped waits vs Developed's 30-47s
- Agent learned to activate pedestrian phases proactively

###### Policy Implications

**Alignment with Sustainable Transport Goals:**

1. **Vision Zero:** 0 safety violations across all scenarios
2. **Mode Shift Incentive:** Reduced bike/ped wait times encourage non-car modes
3. **Transit Priority:** 74% bus improvement supports public transport
4. **Equity:** Vulnerable road users receive superior service

**Deployment Considerations:**

- **Urban core:** Excellent fit (high bike/ped/bus volumes)
- **Car-dominated suburbs:** May face political resistance due to car delays
- **Mixed-use districts:** Ideal application (balanced multimodal demand)

##### Safety Analysis

###### Safety Violations: DRL Agent Performance

**Zero Violations Across All Scenarios:**

The DRL agent achieved **perfect safety performance** with **0 violations** in all 30 test scenarios, representing
300,000 seconds (83.3 hours) of continuous simulation time across diverse traffic conditions.

**Safety Constraint Implementation:**

The DRL agent's safety record stems from three key mechanisms:

1. **Minimum Green Time Enforcement:**

    - Phase-specific minimum green times (5-10s) prevent premature phase changes
    - Ensures vehicles/pedestrians have sufficient crossing time
    - Blocked action penalty (-3.0) discourages unsafe timing violations

2. **Mandatory Transition Phases:**

    - Yellow clearance (3s) + All-red clearance (2s) between all phase changes
    - Total 5-second safety buffer for intersection clearing
    - Agent cannot bypass these automatic safety transitions

3. **Phase Transition Logic:**
    - Proper cycle enforcement (P1‚ÜíP2‚ÜíP3‚ÜíP4‚ÜíP1)
    - Prevents conflicting movements (e.g., cross-traffic green simultaneously)
    - Centralized control ensures both signals change in coordination

**Testing Rigor:**

| Scenario Type     | Scenarios Tested | Simulation Time | Total Violations |
| ----------------- | ---------------- | --------------- | ---------------- |
| Pr (Car-varying)  | 10               | 100,000s        | 0                |
| Bi (Bike-varying) | 10               | 100,000s        | 0                |
| Pe (Ped-varying)  | 10               | 100,000s        | 0                |
| **Total**         | **30**           | **300,000s**    | **0**            |

**Extreme Condition Validation:**

The agent maintained perfect safety even under stress conditions:

- **High car volume:** Pr_9 (1000 cars/hr) - 0 violations
- **High bicycle volume:** Bi_9 (1000 bikes/hr) - 0 violations
- **High pedestrian volume:** Pe_9 (1000 peds/hr) - 0 violations
- **Mixed high demand:** Pr_3, Bi_3, Pe_3 (all 400/400/400) - 0 violations

**Comparison Context:**

While Reference and Developed control safety data are not available for direct comparison, the DRL agent's perfect
safety record demonstrates that:

1. **Reward engineering was effective** - Safety constraints were properly encoded
2. **Training was successful** - Agent learned to respect safety rules without exceptions
3. **Generalization occurred** - Zero violations across unseen test scenarios proves robust policy learning
4. **No safety-performance trade-off** - Agent achieved multimodal improvements WITHOUT compromising safety

###### Safety as a Core Strength

The **zero-violation record** strengthens the case for DRL agent significantly:

1. **Real-world deployability:** Demonstrates DRL can meet stringent safety requirements
2. **Policy confidence:** Municipalities require proven safety before adopting AI control
3. **Regulatory compliance:** Meets traffic signal safety standards (e.g., MUTCD, Vienna Convention)
4. **Risk mitigation:** Eliminates primary concern about AI-based traffic control

**Bottom Line:** The DRL agent's perfect safety performance across 83.3 hours of diverse traffic conditions proves that
multimodal equity optimization can be achieved **without any safety compromise**, addressing the most critical barrier
to real-world deployment of AI traffic control systems.

##### Overall Performance Assessment

##### Strengths (DRL Agent)

1. **Exceptional multimodal equity:** Bicycles (50% better), Pedestrians (82% better), Buses (74% better) compared to
   Developed control

2. **Vulnerable road user priority:** Consistently serves bikes/peds/buses with minimal delays - 24s bikes, 3s peds, 4s
   buses average

3. **High bicycle volumes:** Handles Bi_7-9 scenarios dramatically better (40-45s vs 66-205s Developed control) - 67.6%
   improvement at extreme demand

4. **High pedestrian volumes:** Pe_7-9 scenarios show 89% improvement (4s avg vs 37s Developed) when baseline struggles
   most

5. **Bus priority:** Consistent 2-4s waits across most scenarios vs 10-18s for Developed control

6. **Perfect safety record:** Zero violations across all 30 scenarios (300,000s simulation time)

7. **Scalability validated:** Maintains performance from 100 to 1000 veh/hr across all modes

8. **Policy alignment:** Demonstrates car-reduction incentive through superior alternative mode service (Vision Zero,
   sustainable transport goals)

##### Weaknesses (DRL Agent)

1. **Car waiting times:** 20% worse than Developed control (44s vs 37s average)

2. **High car volumes:** Struggles with Pr_4-9 scenarios (45-51s vs 35-48s Developed) - most significant at mid-high car
   demand (500-700 veh/hr)

3. **Bi scenarios car performance:** Shows 18-31% car degradation when bicycle volume increases (Bi_0-9: 36-50s vs
   32-43s Developed)

4. **Pe scenarios car performance:** Pe_4-6 shows highest car degradation (+33.6% vs Developed) when agent prioritizes
   mid-high pedestrian demand

5. **Trade-off optimization:** Prioritizes bikes/peds/buses over cars - +7.3s car delay for -24.3s bike, -13.9s ped,
   -12.0s bus gains

6. **Mixed high demand:** Pr_4-6 shows agent struggles to balance competing high demands (cars + bikes + peds all
   elevated)

##### Test Scenario Definitions (MSc Thesis)

All 30 test scenarios use consistent bus frequency (every 15 minutes) with varying volumes for cars, bicycles, and
pedestrians.

###### Pr Scenarios (Varying Private Car Volumes)

Constant: 400 bicycles/hr, 400 pedestrians/hr

| Scenario | Cars/hr | Bicycles/hr | Pedestrians/hr | Buses       |
| -------- | ------- | ----------- | -------------- | ----------- |
| Pr_0     | 100     | 400         | 400            | every_15min |
| Pr_1     | 200     | 400         | 400            | every_15min |
| Pr_2     | 300     | 400         | 400            | every_15min |
| Pr_3     | 400     | 400         | 400            | every_15min |
| Pr_4     | 500     | 400         | 400            | every_15min |
| Pr_5     | 600     | 400         | 400            | every_15min |
| Pr_6     | 700     | 400         | 400            | every_15min |
| Pr_7     | 800     | 400         | 400            | every_15min |
| Pr_8     | 900     | 400         | 400            | every_15min |
| Pr_9     | 1000    | 400         | 400            | every_15min |

###### Bi Scenarios (Varying Bicycle Volumes)

Constant: 400 cars/hr, 400 pedestrians/hr

| Scenario | Cars/hr | Bicycles/hr | Pedestrians/hr | Buses       |
| -------- | ------- | ----------- | -------------- | ----------- |
| Bi_0     | 400     | 100         | 400            | every_15min |
| Bi_1     | 400     | 200         | 400            | every_15min |
| Bi_2     | 400     | 300         | 400            | every_15min |
| Bi_3     | 400     | 400         | 400            | every_15min |
| Bi_4     | 400     | 500         | 400            | every_15min |
| Bi_5     | 400     | 600         | 400            | every_15min |
| Bi_6     | 400     | 700         | 400            | every_15min |
| Bi_7     | 400     | 800         | 400            | every_15min |
| Bi_8     | 400     | 900         | 400            | every_15min |
| Bi_9     | 400     | 1000        | 400            | every_15min |

###### Pe Scenarios (Varying Pedestrian Volumes)

Constant: 400 cars/hr, 400 bicycles/hr

| Scenario | Cars/hr | Bicycles/hr | Pedestrians/hr | Buses       |
| -------- | ------- | ----------- | -------------- | ----------- |
| Pe_0     | 400     | 400         | 100            | every_15min |
| Pe_1     | 400     | 400         | 200            | every_15min |
| Pe_2     | 400     | 400         | 300            | every_15min |
| Pe_3     | 400     | 400         | 400            | every_15min |
| Pe_4     | 400     | 400         | 500            | every_15min |
| Pe_5     | 400     | 400         | 600            | every_15min |
| Pe_6     | 400     | 400         | 700            | every_15min |
| Pe_7     | 400     | 400         | 800            | every_15min |
| Pe_8     | 400     | 400         | 900            | every_15min |
| Pe_9     | 400     | 400         | 1000           | every_15min |

##### Average Waiting Time - 4 Modes (MSc Thesis)

##### Average Waiting Time for Private Cars (seconds)

| Scenario name | Reference control (seconds) | Developed control (seconds) |
| ------------- | --------------------------- | --------------------------- |
| Pr_0          | 19                          | 29                          |
| Pr_1          | 24                          | 31                          |
| Pr_2          | 23                          | 32                          |
| Pr_3          | 21                          | 37                          |
| Pr_4          | 22                          | 35                          |
| Pr_5          | 23                          | 38                          |
| Pr_6          | 24                          | 40                          |
| Pr_7          | 26                          | 42                          |
| Pr_8          | 26                          | 42                          |
| Pr_9          | 26                          | 48                          |
| Bi_0          | 22                          | 33                          |
| Bi_1          | 22                          | 32                          |
| Bi_2          | 22                          | 32                          |
| Bi_3          | 21                          | 37                          |
| Bi_4          | 22                          | 36                          |
| Bi_5          | 21                          | 38                          |
| Bi_6          | 21                          | 40                          |
| Bi_7          | 21                          | 40                          |
| Bi_8          | 22                          | 41                          |
| Bi_9          | 23                          | 43                          |
| Pe_0          | 22                          | 27                          |
| Pe_1          | 22                          | 30                          |
| Pe_2          | 22                          | 34                          |
| Pe_3          | 21                          | 37                          |
| Pe_4          | 23                          | 35                          |
| Pe_5          | 21                          | 36                          |
| Pe_6          | 21                          | 38                          |
| Pe_7          | 22                          | 38                          |
| Pe_8          | 21                          | 39                          |
| Pe_9          | 21                          | 36                          |

##### Average Waiting Time for Bicycles (seconds)

| Scenario name | Reference control (seconds) | Developed control (seconds) |
| ------------- | --------------------------- | --------------------------- |
| Pr_0          | 316                         | 32                          |
| Pr_1          | 348                         | 31                          |
| Pr_2          | 232                         | 31                          |
| Pr_3          | 158                         | 33                          |
| Pr_4          | 64                          | 31                          |
| Pr_5          | 54                          | 32                          |
| Pr_6          | 48                          | 35                          |
| Pr_7          | 48                          | 35                          |
| Pr_8          | 40                          | 35                          |
| Pr_9          | 37                          | 38                          |
| Bi_0          | 25                          | 29                          |
| Bi_1          | 34                          | 28                          |
| Bi_2          | 48                          | 31                          |
| Bi_3          | 158                         | 33                          |
| Bi_4          | 267                         | 35                          |
| Bi_5          | 369                         | 40                          |
| Bi_6          | 507                         | 55                          |
| Bi_7          | 647                         | 66                          |
| Bi_8          | 598                         | 122                         |
| Bi_9          | 667                         | 205                         |
| Pe_0          | 116                         | 25                          |
| Pe_1          | 108                         | 28                          |
| Pe_2          | 152                         | 31                          |
| Pe_3          | 158                         | 33                          |
| Pe_4          | 102                         | 33                          |
| Pe_5          | 113                         | 33                          |
| Pe_6          | 93                          | 35                          |
| Pe_7          | 94                          | 35                          |
| Pe_8          | 88                          | 35                          |
| Pe_9          | 123                         | 34                          |

##### Average Waiting Time for Pedestrians (seconds)

| Scenario name | Reference control (seconds) | Developed control (seconds) |
| ------------- | --------------------------- | --------------------------- |
| Pr_0          | 129                         | 11                          |
| Pr_1          | 69                          | 11                          |
| Pr_2          | 25                          | 11                          |
| Pr_3          | 14                          | 12                          |
| Pr_4          | 13                          | 12                          |
| Pr_5          | 13                          | 13                          |
| Pr_6          | 12                          | 12                          |
| Pr_7          | 12                          | 13                          |
| Pr_8          | 12                          | 14                          |
| Pr_9          | 12                          | 14                          |
| Bi_0          | 12                          | 10                          |
| Bi_1          | 14                          | 11                          |
| Bi_2          | 13                          |                             |
| Bi_3          | 14                          | 12                          |
| Bi_4          | 13                          | 12                          |
| Bi_5          | 15                          | 13                          |
| Bi_6          | 15                          | 13                          |
| Bi_7          | 15                          | 15                          |
| Bi_8          | 17                          | 17                          |
| Bi_9          | 17                          | 15                          |
| Pe_0          | 9                           | 10                          |
| Pe_1          | 9                           | 9                           |
| Pe_2          | 11                          | 11                          |
| Pe_3          | 14                          | 12                          |
| Pe_4          | 48                          | 13                          |
| Pe_5          | 87                          | 17                          |
| Pe_6          | 94                          | 22                          |
| Pe_7          | 105                         | 30                          |
| Pe_8          | 111                         | 35                          |
| Pe_9          | 126                         | 47                          |

##### Average Waiting Time for Buses (seconds)

| Scenario name | Reference control (seconds) | Developed control (seconds) |
| ------------- | --------------------------- | --------------------------- |
| Pr_0          | 26                          | 9                           |
| Pr_1          | 28                          | 12                          |
| Pr_2          | 26                          | 11                          |
| Pr_3          | 26                          | 12                          |
| Pr_4          | 22                          | 24                          |
| Pr_5          | 32                          | 18                          |
| Pr_6          | 21                          | 14                          |
| Pr_7          | 21                          | 21                          |
| Pr_8          | 20                          | 32                          |
| Pr_9          | 24                          | 27                          |
| Bi_0          | 24                          | 14                          |
| Bi_1          | 23                          | 18                          |
| Bi_2          | 23                          | 16                          |
| Bi_3          | 26                          | 12                          |
| Bi_4          | 24                          | 14                          |
| Bi_5          | 27                          | 13                          |
| Bi_6          | 31                          | 17                          |
| Bi_7          | 19                          | 16                          |
| Bi_8          | 22                          | 15                          |
| Bi_9          | 24                          | 17                          |
| Pe_0          | 31                          | 7                           |
| Pe_1          | 21                          | 10                          |
| Pe_2          | 21                          | 12                          |
| Pe_3          | 26                          | 12                          |
| Pe_4          | 22                          | 18                          |
| Pe_5          | 30                          | 17                          |
| Pe_6          | 28                          | 17                          |
| Pe_7          | 29                          | 25                          |
| Pe_8          | 29                          | 17                          |
| Pe_9          | 22                          | 11                          |

##### Table DRL Agent Test Results - Average Waiting Times (seconds)

| Scenario | Car Wait (s) | Bicycle Wait (s) | Pedestrian Wait (s) | Bus Wait (s) | Safety Violations |
| -------- | ------------ | ---------------- | ------------------- | ------------ | ----------------- |
| Pr_0     | 17.63        | 13.70            | 4.79                | 2.09         | 0                 |
| Pr_1     | 24.29        | 15.04            | 3.00                | 1.94         | 0                 |
| Pr_2     | 33.96        | 19.94            | 5.72                | 2.04         | 0                 |
| Pr_3     | 37.83        | 21.18            | 3.22                | 1.84         | 0                 |
| Pr_4     | 51.07        | 22.40            | 2.76                | 12.08        | 0                 |
| Pr_5     | 50.83        | 23.92            | 4.86                | 10.30        | 0                 |
| Pr_6     | 48.94        | 20.34            | 2.08                | 12.79        | 0                 |
| Pr_7     | 46.55        | 14.86            | 1.16                | 10.87        | 0                 |
| Pr_8     | 44.81        | 13.46            | 0.92                | 14.54        | 0                 |
| Pr_9     | 45.88        | 17.14            | 1.73                | 13.47        | 0                 |
| Bi_0     | 40.14        | 7.05             | 2.01                | 2.31         | 0                 |
| Bi_1     | 45.72        | 9.16             | 1.48                | 2.00         | 0                 |
| Bi_2     | 45.71        | 13.98            | 3.07                | 2.63         | 0                 |
| Bi_3     | 43.86        | 25.49            | 5.21                | 3.25         | 0                 |
| Bi_4     | 49.48        | 32.48            | 2.29                | 3.87         | 0                 |
| Bi_5     | 36.28        | 28.51            | 2.21                | 1.41         | 0                 |
| Bi_6     | 48.98        | 43.05            | 5.21                | 2.42         | 0                 |
| Bi_7     | 42.14        | 39.49            | 2.91                | 2.68         | 0                 |
| Bi_8     | 45.80        | 45.33            | 1.87                | 2.26         | 0                 |
| Bi_9     | 45.55        | 42.40            | 3.82                | 1.69         | 0                 |
| Pe_0     | 40.74        | 17.60            | 1.47                | 3.35         | 0                 |
| Pe_1     | 39.59        | 19.58            | 2.02                | 2.67         | 0                 |
| Pe_2     | 37.45        | 20.70            | 2.25                | 2.73         | 0                 |
| Pe_3     | 41.02        | 22.96            | 2.60                | 2.42         | 0                 |
| Pe_4     | 52.57        | 23.97            | 2.18                | 1.20         | 0                 |
| Pe_5     | 47.86        | 21.39            | 2.92                | 2.12         | 0                 |
| Pe_6     | 45.21        | 24.23            | 4.44                | 2.17         | 0                 |
| Pe_7     | 42.36        | 20.14            | 3.13                | 2.77         | 0                 |
| Pe_8     | 42.23        | 18.97            | 4.84                | 3.47         | 0                 |
| Pe_9     | 49.94        | 23.97            | 3.99                | 4.60         | 0                 |

---

---

# Traffic Demand Generation

##### Test Scenario Design

To comprehensively evaluate the DRL agent's performance across diverse traffic conditions, we developed **30 systematic
test scenarios** organized into three categories based on predominant mode:

##### Scenario Categories

**1. Private Car Priority (Pr_0 to Pr_9):**

- Variable car demand: 100‚Äì1000 vehicles/hour (increments of 100)
- Constant bicycle demand: 400 bicycles/hour
- Constant pedestrian demand: 400 pedestrians/hour
- Bus service: Every 15 minutes

**2. Bicycle Priority (Bi_0 to Bi_9):**

- Constant car demand: 400 vehicles/hour
- Variable bicycle demand: 100‚Äì1000 bicycles/hour (increments of 100)
- Constant pedestrian demand: 400 pedestrians/hour
- Bus service: Every 15 minutes

**3. Pedestrian Priority (Pe_0 to Pe_9):**

- Constant car demand: 400 vehicles/hour
- Constant bicycle demand: 400 bicycles/hour
- Variable pedestrian demand: 100‚Äì1000 pedestrians/hour (increments of 100)
- Bus service: Every 15 minutes

This systematic variation allows analysis of agent behavior under:

- **Low demand** (100/hour): Underutilized intersection capacity
- **Moderate demand** (400/hour): Balanced multi-modal flow
- **High demand** (700‚Äì1000/hour): Saturation conditions and capacity constraints

Each scenario runs for **10,000 simulation seconds** to ensure statistical significance and capture long-term agent
behavior patterns.

###### Traffic Flow Distribution

Vehicle routing at the intersection follows realistic distribution patterns based on observed urban traffic
characteristics:

###### Directional Flow Ratios

**Through Movement Dominance:**

- **Straight-through traffic:** 80% of total flow
- **Left turns:** 10% of total flow
- **Right turns:** 10% of total flow

This 80-10-10 distribution reflects typical arterial road patterns where through movements dominate, while turning
movements create secondary demand.

###### Major vs Minor Roadway Split

The intersection features asymmetric approach volumes:

- **Minor roadway volume:** 25% of major roadway volume
- **Major roadway volume:** Primary traffic flow (100%)

This 4:1 ratio simulates realistic urban grid networks where primary arterials carry significantly higher volumes than
collector roads. The asymmetry challenges the DRL agent to balance efficiency on high-volume approaches with equity for
lower-volume approaches. Bicycle and pedestrian flows follow uniform distributions across approaches, with demand
specified per scenario (100‚Äì1000/hour). Bus routes traverse the major arterial (north-south) with deterministic
15-minute headways (4 buses/hour), enabling evaluation of bus priority mechanisms (Skip-to-P1 action).

This traffic generation methodology ensures:

1. **Reproducibility:** Deterministic flows enable exact scenario replication
2. **Realism:** Distribution ratios match observed urban traffic patterns
3. **Comprehensiveness:** 30 scenarios span low-to-saturation demand across all modes
4. **Challenge diversity:** Mode-specific scenarios test agent adaptability

---

---

# Phase Structure and Durations

##### Four-Phase Signal Control Strategy

The DRL agent operates a **four-phase signal control structure** designed to serve all transportation modes equitably
while maintaining efficient vehicular throughput. Figure below illustrates the phase sequence and permitted movements.

<div align="center">
<img src="../images/1/phase_structure.png" alt="Phase Structure Diagram" width="400" height=auto/>
<p align="center">figure: Four-phase signal control structure showing permitted movements and transitions. Yellow intervals separate conflicting phases to ensure safety.</p>
</div>

##### Phase Definitions

**Phase 1 (P1) ‚Äì Major Arterial Through:**

- **Permitted movements:** North-south through traffic, right turns
- **Primary users:** Private vehicles, buses (bus priority lanes on major arterial)
- **Typical duration:** 12‚Äì44 seconds
- **Role:** Main traffic clearance phase serving highest-volume movements

**Phase 2 (P2) ‚Äì Minor Roadway Through:**

- **Permitted movements:** East-west through traffic, right turns
- **Primary users:** Private vehicles from collector roads
- **Typical duration:** 5‚Äì15 seconds
- **Role:** Service lower-volume approaches, prevent starvation

**Phase 3 (P3) ‚Äì Bicycle Phase:**

- **Permitted movements:** Dedicated bicycle crossings (all approaches)
- **Primary users:** Cyclists
- **Typical duration:** 7‚Äì24 seconds
- **Role:** Safe protected crossing time for vulnerable road users

**Phase 4 (P4) ‚Äì Pedestrian Phase:**

- **Permitted movements:** All pedestrian crosswalks
- **Primary users:** Pedestrians
- **Typical duration:** 4‚Äì12 seconds
- **Role:** Exclusive pedestrian clearance, highest safety priority

##### Phase Duration Hierarchy

The DRL agent must respect a **hierarchical duration structure** that balances operational efficiency, safety
constraints, and traffic responsiveness. Five duration thresholds govern each phase:

| Phase               | Min Green | Stability | Next Bonus | Consecutive Continue | Max Green |
| ------------------- | --------- | --------- | ---------- | -------------------- | --------- |
| **P1 (Major)**      | 8s        | 10s       | 12s        | 30s                  | 44s       |
| **P2 (Minor)**      | 3s        | 4s        | 5s         | 10s                  | 15s       |
| **P3 (Bicycle)**    | 5s        | 6s        | 7s         | 15s                  | 24s       |
| **P4 (Pedestrian)** | 2s        | 3s        | 4s         | 8s                   | 12s       |

##### Duration Threshold Rationale

**1. Minimum Green Time (Safety Constraint):**

- **Purpose:** Ensures yellow clearance interval + minimum service time
- **Derivation:** Based on approach speed limits and pedestrian/cyclist crossing times
- **Example (P1):** 8s = 4s yellow + 4s minimum green
- **Enforcement:** Agent cannot transition before min green elapsed (hard constraint)

**2. Stability Threshold (Reward Incentive):**

- **Purpose:** Discourages premature phase changes that waste clearance time
- **Rationale:** Transitioning before stability wastes 4s yellow; holding ‚â•10s amortizes this cost
- **Reward mechanism:** Stability bonus (+0.05) awarded when Continue action holds phase ‚â• stability threshold
- **Impact:** Encourages agent to commit to phase decisions rather than oscillate

**3. Next Bonus Threshold (Efficiency Incentive):**

- **Purpose:** Rewards timely phase advancement after adequate service time
- **Rationale:** Prevents phase from holding too long when demand served; promotes flow efficiency
- **Reward mechanism:** Next action receives bonus (+0.08‚Äì0.11) when duration ‚â• next bonus threshold
- **Impact:** Agent learns optimal phase termination timing

**4. Consecutive Continue Threshold (Diversity Enforcement):**

- **Purpose:** Prevents agent from camping indefinitely in high-throughput phases (especially P1)
- **Rationale:** Ensures all modes receive service; prevents starvation of secondary modes
- **Penalty mechanism:** Large penalty (‚àí0.15 to ‚àí0.30) if Continue selected beyond threshold
- **Impact:** Forces periodic cycling through all phases

**5. Maximum Green Time (Capacity Constraint):**

- **Purpose:** Hard upper limit on phase duration
- **Rationale:** Prevents complete starvation; guarantees service intervals for all modes
- **Enforcement:** Agent cannot extend phase beyond max green (hard constraint)
- **Design:** Set at ~3√ó next bonus threshold to allow adaptive extensions under high demand

##### Design Philosophy

The duration hierarchy embodies three key principles:

**Safety First:** Minimum green times derived from traffic engineering standards (ITE, MUTCD) ensure adequate clearance
and crossing times. These are non-negotiable hard constraints.

**Efficiency Through Incentives:** Rather than rigid fixed timing, the reward structure (stability bonus, next bonus)
guides the agent toward efficient phase durations while preserving adaptability to real-time conditions.

**Equity Through Constraints:** Consecutive continue thresholds and maximum green limits prevent the agent from
optimizing solely for high-volume modes. These constraints ensure vulnerable road users (cyclists, pedestrians) receive
regular service opportunities.

##### Agent Learning Implications

During training, the agent discovers optimal phase durations within these bounds:

- **P1 (Major arterial):** Agent learns to hold 12‚Äì30s depending on queue lengths, typically releasing around 24s
  (observed in testing)
- **P2 (Minor roadway):** Agent learns brief service (5‚Äì10s) sufficient for minor approach clearance
- **P3 (Bicycle):** Agent adapts 7‚Äì15s based on bicycle detector occupancy
- **P4 (Pedestrian):** Agent provides minimum safe crossing (4‚Äì8s), extending only under high pedestrian demand

This hierarchical structure enables the agent to learn **context-sensitive timing** while guaranteeing safety and
equity‚Äîa balance unattainable with fixed-time control.

---

---

# Network Topology and Detector Placement in the DRL Control

#### Network Topology

- **Urban Corridor Configuration**

The simulation network represents a typical urban arterial corridor comprising two signalized intersections separated by
300 meters along a major arterial roadway. Each intersection serves as a junction point where a minor cross-street
intersects the major arterial perpendicularly, creating a classic urban grid configuration. This spatial arrangement
enables the investigation of corridor-level traffic coordination strategies, where upstream signal operations influence
downstream traffic conditions through platoon progression and queue spillback dynamics.

The 300-meter separation distance was selected to reflect common urban intersection spacing in medium-density
developments. This distance permits meaningful signal coordination while remaining sufficiently short to maintain
practical relevance for green wave timing and platoon preservation. The corridor exhibits no intermediate traffic
generation or termination points between the two intersections, with the exception of designated bus stops positioned
downstream of each intersection. This closed-system design isolates the effects of signal control decisions from
exogenous traffic influences, facilitating clearer attribution of performance outcomes to control strategies.

- **Major Arterial Infrastructure**

The major arterial roadway employs a dual-lane configuration at intersection approaches, providing dedicated spatial
allocation for distinct movement types. The leftmost vehicular lane functions exclusively for left-turning movements,
while the right lane accommodates through traffic with permissive right turns on red where geometric conditions permit.
This segregation of turning movements from through traffic reduces conflict points and enhances intersection capacity by
enabling concurrent service of compatible movement streams.

Downstream of each intersection, a 15-meter bus bay is constructed along the right lane, providing dedicated stopping
space for public transit vehicles. This infrastructure prevents buses from obstructing through traffic during passenger
boarding and alighting operations, thereby maintaining arterial flow continuity. The bus bay placement immediately
downstream of the intersection ensures that buses experience minimal delay from signal control, as they traverse the
intersection during green phases before decelerating to the stop location.

Between the two intersections, the major arterial transitions to a single through lane in each direction over a 90-meter
link section. This configuration represents the typical narrowing of roadway cross-section between intersections common
in constrained urban environments. The single-lane section necessitates that vehicles merge from the dual-lane departure
configuration before reaching the downstream intersection approach, creating potential for merge-related delays that the
signal control system must accommodate.

- **Minor Road Configuration**

The minor cross-streets employ infrastructure geometry comparable to the major arterial, albeit at reduced scale
reflective of lower traffic demand. Each minor road approach features dual entry lanes at the intersection, with
left-turn and through movements similarly segregated. The minor roads experience traffic volumes approximately
one-quarter that of the major arterial, consistent with typical hierarchical road network designs where collector
streets feed traffic to and from primary arterials.

The asymmetric demand relationship between major and minor movements creates an inherent tension in signal timing
optimization. Allocating excessive green time to minor phases reduces major arterial efficiency and contradicts the
hierarchical network function, while insufficient minor phase service generates excessive delays and potential spillback
onto upstream networks. This trade-off represents a fundamental challenge in multimodal signal optimization that the
control system must resolve.

- **Bicycle Infrastructure**

The network incorporates comprehensive bicycle infrastructure designed to accommodate both through and turning movements
for non-motorized vehicles. Bicycle lanes are implemented as dual-lane facilities parallel to vehicular lanes, with
dedicated left-turn lanes positioned adjacent to the intersection. This configuration mirrors the vehicular lane
arrangement, providing cyclists with comparable movement flexibility and dedicated right-of-way.

The dual bicycle lane design enables overtaking maneuvers, addressing the speed heterogeneity characteristic of bicycle
traffic where faster cyclists can pass slower riders without requiring use of vehicular lanes. The leftmost bicycle lane
serves exclusively left-turning cyclists, while the rightmost lane accommodates through movements with permissive right
turns. Bicycle detectors positioned 15 meters upstream from stop lines provide equal travel time to the intersection for
both bicycles and motorized vehicles, enabling coordinated actuation logic that accounts for bicycle approach speeds
approximately half those of motorized vehicles.

- **Pedestrian Facilities**

Pedestrian infrastructure consists of dedicated sidewalk facilities positioned laterally adjacent to the roadway
cross-section, with marked crosswalks at all intersection legs. The pedestrian network supports straight crossing
movements at each intersection, enabling mobility between all four quadrants of the intersection. Crosswalk widths
accommodate typical pedestrian volumes while maintaining sufficient separation from vehicular travel lanes to enhance
safety.

Pedestrian detection infrastructure employs push button at the stop lines to inform their presence, capturing queue
formation behavior at signal heads. This pedestrian push button enables detection of stable pedestrian queues while
maintaining sufficient proximity to the crossing to represent immediate demand conditions. The detection system monitors
both pedestrian presence and movement speed, with stopped or slowly moving pedestrians indicating service demand.

- **Network Connectivity and Boundary Conditions**

The network periphery incorporates traffic generation and absorption zones that simulate realistic arrival and departure
patterns without explicit representation of the broader street network. Vehicle arrivals follow Poisson stochastic
processes with parametrically varied mean rates to represent different demand scenarios. Departure zones absorb vehicles
exiting the corridor, preventing artificial congestion from boundary effects.

Bus services operate on fixed schedules with 15-minute headways in both directions along the major arterial,
representing typical urban transit service frequencies. Buses follow predetermined routes that traverse both
intersections, generating predictable demand patterns that the signal control system can anticipate and accommodate
through priority strategies.

The closed-corridor configuration with controlled entry and exit points enables systematic investigation of signal
control performance across diverse demand conditions while maintaining experimental control over confounding variables.
This network design balances geometric realism with analytical tractability, providing a representative testbed for
multimodal signal optimization strategies applicable to typical urban arterial corridors.

#### Detector Infrastructure

The deep reinforcement learning (DRL) control system employs the identical detector infrastructure established in the
developed multimodal control. This design choice ensures fair comparison between control strategies by eliminating
infrastructure-related confounding variables. The detector layout comprises three types of induction loop detectors
strategically positioned to capture multimodal traffic conditions at both signalized intersections.

- **Vehicle Detection Infrastructure**

Vehicle queue detection utilizes single-loop induction detectors positioned 30 meters upstream from the stop line on
each vehicular approach. Although the infrastructure includes dual detectors at both 30 meters and 100 meters per the
original design, only the 30-meter detectors are actively employed in both the developed control and the DRL control.
The 100-meter detectors remain installed but unused, ensuring consistency between control strategies. These 30-meter
detectors operate with a detection window of 3 seconds, meaning that if a vehicle has crossed the detector within the
previous 3 seconds, the approach is classified as having queue presence. This binary classification approach provides
sufficient information granularity for both rule-based actuation logic and neural network state representation.

The 30-meter placement distance was determined based on the maximum permitted speed of 40 kilometers per hour for
motorized vehicles, yielding an approximate travel time of 2.7 seconds from detector activation to stop line arrival.
This positioning enables anticipatory phase management while maintaining safety margins for minimum green time
requirements.

- **Bicycle Detection Infrastructure**

Bicycle queue detection employs dedicated single-loop detectors positioned 15 meters upstream from the stop line on each
approach. The reduced detection distance relative to vehicular detectors accounts for the lower maximum speed of
bicycles, defined as 20 kilometers per hour in the simulation environment. This positioning ensures approximately
equivalent travel times from detector activation to intersection arrival for both bicycles and motorized vehicles,
thereby maintaining temporal consistency in the actuation logic across different modal types.

The detection methodology mirrors that of vehicular detectors, utilizing a 3-second detection window to establish binary
queue presence indicators. This uniform detection threshold across modal types simplifies the state representation while
capturing the essential traffic conditions necessary for control decisions.

- **Pedestrian Detection Infrastructure**

The system monitors pedestrian performance using SUMO's TRACI person API get their waiting time and speed. Pedestrian
waiting times are collected continuously and incorporated into the reward function to encourage the DRL agent to
minimize pedestrian delays. Similar to traditional push-button actuation, this approach provides continuous feedback on
pedestrian service quality near to the signalized intersections to generate demand signals.

The decision to maintain identical detector infrastructure between the developed control and the DRL control establishes
methodological rigor in the comparative evaluation. Both systems receive equivalent observational information from the
intersection environment, ensuring that performance differences arise from the control algorithms themselves rather than
from disparities in sensing capabilities. The developed control demonstrates the performance achievable through
expert-designed actuation rules applied to these detector measurements, while the DRL control reveals the potential
performance gains obtainable through learned policies operating on the same detector infrastructure.

This infrastructure consistency extends beyond mere detector positioning to encompass detection window thresholds, speed
limits, and demand activation criteria. By preserving these parameters from the validated developed control, the DRL
implementation inherits a proven sensing framework while introducing algorithmic innovation in the decision-making
layer. This approach facilitates direct attribution of performance outcomes to the respective control methodologies,
strengthening the validity of comparative conclusions.

\*\*Note: Network Topology and Detector Placement in the DRL Control is same as the rule-based Developed except for the
pedestrians

---

---

# Integration of the Detectors with DRL Architecture

##### State Vector Construction

The detector measurements constitute a substantial portion of the DRL agent's observational state space. For each of the
two signalized intersections, the state representation incorporates multiple detector-derived features that collectively
capture the multimodal traffic conditions.

###### State Vector Composition

The complete state vector $\mathbf{s}_t \in \mathbb{R}^{32}$ comprises **16 features per intersection** (2 intersections
√ó 16 features = 32 total dimensions). For each intersection $i \in {1, 2}$, the state includes:

**1. Phase Context Features (5 dimensions):**

- **Phase encoding** (4 dimensions): One-hot representation of current main green phase

$$
\phi_{phase} = [p_1, p_2, p_3, p_4] \in {0,1}^4
$$

where exactly one element equals 1, indicating Phase 1, 2, 3, or 4

- **Phase duration** (1 dimension): Normalized time in current phase

$$
\phi_{duration} = \min\left(\frac{d^{(i)}}{60.0}, 1.0\right) \in [0, 1]
$$

where $d^{(i)}$ is the current phase duration in seconds

**2. Vehicle Queue Detection Features (4 dimensions):**

Binary indicators derived from inductive loop detectors on vehicle approach lanes:

$$
\phi_{veh} = [q_1^{veh}, q_2^{veh}, q_3^{veh}, q_4^{veh}] \in {0,1}^4
$$

where:

$$
q_j^{veh} = \begin{cases} 1 & \text{if detector } j \text{ activated within last 3 seconds} \\ 0 & \text{otherwise} \end{cases}
$$

Each dimension corresponds to one of four approach directions, providing spatial awareness of vehicular demand.

**3. Bicycle Queue Detection Features (4 dimensions):**

Binary indicators from dedicated bicycle detection infrastructure:

$$
\phi_{bike} = [q_1^{bike}, q_2^{bike}, q_3^{bike}, q_4^{bike}] \in {0,1}^4
$$

Using the same 3-second activation window logic, these features capture bicycle presence on protected lanes.

**4. Bus Priority Features (2 dimensions):**

Features specific to public transit prioritization:

- **Bus presence** (1 dimension): Binary indicator for buses in priority lanes

$$
\phi_{bus}^{presence} = \begin{cases} 1.0 & \text{if } \exists \text{ bus in priority lanes} \ 0.0 & \text{otherwise} \end{cases}
$$

- **Bus waiting time** (1 dimension): Normalized average waiting time

$$
\phi_{bus}^{wait} = \min\left(\frac{\bar{w}_{bus}}{60.0}, 1.0\right) \in [0, 1]
$$

where $\bar{w}_{bus}$ is the average accumulated waiting time of buses in priority lanes

**5. Temporal Context Feature (1 dimension):**

- **Simulation time** (1 dimension): Normalized episode progress

$$
\phi_{time} = \min\left(\frac{t_{sim}}{T_{max}}, 1.0\right) \in [0, 1]
$$

where $t_{sim}$ is the current simulation time and $T_{max} = 3600$ seconds during training

###### Complete State Vector Structure

For both intersections combined:

$$
\mathbf{s}_t = [\mathbf{s}_t^{(1)}, \mathbf{s}_t^{(2)}] \in \mathbb{R}^{32}
$$

where each intersection's state is:

$$
\mathbf{s}_t^{(i)} = [\phi*{phase}^{(i)}, \phi_{duration}^{(i)}, \phi_{veh}^{(i)}, \phi_{bike}^{(i)}, \phi_{bus}^{(i)}, \phi_{time}^{(i)}] \in \mathbb{R}^{16}
$$

###### Detector Contribution to State Space

**Detector-derived features account for 20 of 32 total state dimensions (62.5%)**, distributed as:

| Feature Category        | Dimensions per Intersection | Total Dimensions | Percentage |
| ----------------------- | --------------------------- | ---------------- | ---------- |
| Vehicle queue detectors | 4                           | 8                | 25.0%      |
| Bicycle queue detectors | 4                           | 8                | 25.0%      |
| Bus presence detection  | 1                           | 2                | 6.25%      |
| Bus waiting time        | 1                           | 2                | 6.25%      |
| **Detector subtotal**   | **10**                      | **20**           | **62.5%**  |
| Phase encoding          | 4                           | 8                | 25.0%      |
| Phase duration          | 1                           | 2                | 6.25%      |
| Simulation time         | 1                           | 2                | 6.25%      |
| **Total state**         | **16**                      | **32**           | **100%**   |

This substantial detector representation (62.5% of state space) underscores the critical role of real-time sensor
information in enabling traffic-responsive control policies.

##### Functional Distinction from Rule-Based Control

The fundamental distinction between traditional actuated control and DRL-based control lies in the **utilization
methodology** of detector information, rather than the detector infrastructure itself.

##### Rule-Based Actuated Control

In conventional actuated systems, detector activations directly trigger **predefined decision rules**:

**Extension Logic:**

$$
\begin{align}
&\textbf{if } (\text{detector\_occupied} \land \text{green\_phase\_active} \land \text{duration} < \text{max\_green}): \\
&\quad \text{EXTEND green phase} \\
&\textbf{else if } (\text{detector\_unoccupied} \land \text{duration} > \text{min\_green}): \\
&\quad \text{TERMINATE green phase and advance to next}
\end{align}
$$

These rules operate on **fixed thresholds** and **immediate local information**:

- Continuous detector occupation ‚Üí extend current phase
- Detector gap-out (no vehicles detected for X seconds) ‚Üí terminate phase
- Minimum/maximum green times ‚Üí hard constraints

**Characteristics:**

- **Deterministic**: Same detector pattern always produces same action
- **Local**: Decisions based on current intersection state only
- **Immediate**: No consideration of future consequences
- **Fixed**: Thresholds and logic predetermined by traffic engineer

##### DRL-Based Learned Control

Conversely, the DRL agent treats detector measurements as **observational features** within a learned decision-making
framework. The deep Q-network receives detector states as input and discovers optimal mappings from detector patterns to
control actions through reinforcement learning.

**Learning Process:**

$$
Q^*(s, a) = \mathbb{E}\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k} \mid s_t = s, a_t = a, \pi^*\right]
$$

The neural network learns to predict long-term cumulative reward for each action given current detector patterns,
implicitly encoding:

- **Temporal patterns**: Queue buildup rates, clearance times
- **Modal interactions**: Car-bicycle-bus conflicts and priorities
- **Coordination effects**: Impact of phase changes on adjacent intersection
- **Strategic timing**: When to serve minor phases vs. maintain major arterial flow

**Characteristics:**

- **Stochastic (during training)**: $\epsilon$-greedy exploration enables diverse experiences
- **Global**: State includes both intersections, enabling coordination
- **Predictive**: Q-values represent expected future cumulative reward
- **Adaptive**: Policy adjusts based on learned traffic patterns

###### Example: Vehicle Queue Response

**Scenario:** Vehicle detector continuously occupied on major arterial approach during Phase 1.

**Rule-Based Response:**

$$
\begin{align}
&\textbf{if } (\text{vehicle\_detector\_occupied} \land \text{phase\_1\_active}): \\
&\quad \text{duration} \leftarrow \text{duration} + 1 \\
&\quad \textbf{if } \text{duration} \geq \text{max\_green\_time}: \\
&\quad\quad \text{FORCE phase change}
\end{align}
$$

Action: Mechanically extend Phase 1 until max green (e.g., 44s), then force change.

**DRL-Based Response:**

The neural network computes Q-values:

$$
\begin{align}
    Q(s, a_0) &= +0.8 \quad \text{(Continue: good, but diminishing returns)} \\
    Q(s, a_1) &= -0.3 \quad \text{(Skip to P1: invalid, already in P1)} \\
    Q(s, a_2) &= +1.2 \quad \text{(Next: advance to serve waiting bicycles on P2)}
\end{align}
$$

Action: Select $a_2$ (Next) despite vehicle queue, because:

1. Phase 1 served adequately (queue clearing observed)
2. Bicycle queue building on Phase 2 (equity penalty accumulating)
3. Bus approaching on minor arterial (prioritization needed)
4. Continuing Phase 1 risks excessive continuation penalty

The agent **overrides** simple queue-extension logic to balance multiple objectives.

##### Holistic State Integration

The neural network architecture learns to interpret detector signals **in concert with other state features**:

##### Multi-Feature Integration

**Phase Context Integration:**

The agent learns phase-dependent detector interpretation:

- Vehicle queue in Phase 1 (major arterial) ‚Üí high priority continuation
- Vehicle queue in Phase 4 (minor left) ‚Üí lower priority, consider advancing

**Temporal Integration:**

Phase duration modulates detector importance:

- Short duration (< min green): Detector queues cannot justify phase change (blocked action)
- Medium duration (optimal window): Detector patterns guide Next vs. Continue decision
- Long duration (approaching max green): Detector queues less influential, strategic advancement prioritized

**Modal Priority Integration:**

Bus presence and waiting time features enable learned transit prioritization:

- $\phi_{bus}^{presence} = 1$ AND $\phi_{bus}^{wait} > 0.15$ ‚Üí Strong incentive for Skip to P1 or Continue P1
- Bus waiting triggers differential reward scaling in Component 10 (Bus Assistance)

**Coordination Integration:**

State includes both intersections simultaneously ($\mathbf{s}_t \in \mathbb{R}^{32}$), enabling learned coordination:

- Detector queues at Intersection 1 influence actions affecting both intersections
- Agent learns implicit green wave timing through trial-and-error
- Centralized control naturally discovers coordination strategies

##### Adaptive Weighting Through Value Functions

Unlike rule-based control with fixed detector thresholds, the DRL agent adaptively weights detector information based on
**learned value functions**:

$$
\frac{\partial Q(s, a; \theta)}{\partial \phi_{veh}^{(i)}} = \text{learned gradient} \neq \text{constant}
$$

The network's learned weights ($\mathbf{W}_1, \mathbf{W}_2, \mathbf{W}_3, \mathbf{W}_4$) determine how strongly each
detector feature influences Q-value predictions. These weights adapt during training to maximize cumulative reward:

- Early training: Random weights, weak detector-action associations

- Mid training: Emerging patterns (e.g., vehicle queue ‚Üí Continue Phase 1)

- Late training: Refined policies (e.g., vehicle queue ‚Üí Continue **unless** excessive duration or high-priority bus
  present)

##### Learning Dynamics and Detector Interpretation

##### Progressive Refinement Through Experience

During the training process, the DRL agent progressively refines its interpretation of detector signals through
**episodic experience accumulation**:

**Phase 1: Random Exploration (Episodes 1-20, $\epsilon \approx 0.8-1.0$)**

- Agent selects mostly random actions regardless of detector states
- Replay buffer accumulates diverse detector-action-outcome associations
- Neural network begins learning basic patterns (e.g., empty detectors in Phase 1 ‚Üí low reward)

**Phase 2: Pattern Discovery (Episodes 20-60, $\epsilon \approx 0.4-0.8$)**

- Temporal difference learning identifies correlations between detector patterns and rewards
- Agent learns: Extended vehicle queue occupancy on major arterial ‚Üí Continue yields positive reward
- Initial strategy: Maximize vehicle throughput (dominant reward component)

**Phase 3: Strategic Refinement (Episodes 60-100, $\epsilon \approx 0.05-0.4$)**

- Agent discovers nuanced detector interpretations:
    - **Override local queues for global coordination**: Detectors show queue, but synchronization reward favors
      advancing
    - **Modal priority learning**: Bus presence detector activates ‚Üí Skip to P1 earns substantial bus assistance bonus
    - **Equity-aware decisions**: Bicycle detector occupancy triggers phase advancement despite vehicle queues
    - **Temporal context**: Same detector pattern yields different optimal actions based on phase duration

**Phase 4: Policy Convergence (Episodes 100+, $\epsilon = 0.05$)**

- Exploitation-dominated policy ($95%$ greedy)
- Stable detector interpretation with consistent action selection
- Residual exploration maintains robustness to unseen traffic patterns

##### Learned Detector Strategies

Through training, the agent develops sophisticated detector interpretation rules:

**Strategy 1: Adaptive Queue Thresholds**

Unlike fixed gap-out timers, the agent learns context-dependent queue sensitivity:

$$
\text{Optimal action} = f(\phi_{veh}, \phi_{bike}, \phi_{bus}, \phi_{duration}, \phi_{phase})
$$

**Strategy 2: Phase-Specific Queue Monitoring**

The agent learns to interpret detector patterns within the current phase. Since vehicles and bicycles are served
simultaneously in each phase (on separate lanes), the agent monitors:

- High vehicle queue + high bicycle queue ‚Üí Continue phase (both modes being served)
- Low vehicle queue + low bicycle queue ‚Üí Advance to next phase (demand satisfied)
- High vehicle queue + low bicycle queue ‚Üí Continue phase (cars still waiting, bikes using green efficiently)

This implicit rule emerges from the waiting time penalty (Components 1-2) which rewards clearing queues before
advancing.

**Strategy 3: Predictive Bus Prioritization**

Bus presence detector activates ‚Üí Agent predicts high future reward for:

1. Continue current phase (if Phase 1 serves bus route)
2. Skip to Phase 1 (if currently in Phase 2/3/4)

This strategy emerges from Component 10 (Bus Assistance: +0.3 bonus for Skip with waiting bus).

**Strategy 4: Detector-Informed Phase Cycling**

The agent learns optimal phase service durations implicitly from detector patterns:

- Detector occupancy drops below threshold ‚Üí Advance to next phase
- All detectors clear across current phase ‚Üí Early phase termination (minimize wasteful green)
- Persistent detector occupancy ‚Üí Extended green time (continue serving demand)

These thresholds are **not hardcoded** but learned through value function approximation. The system uses a 3-second
detection window (detector triggered within last 3 seconds = queue present).

##### Environment-Specific Adaptation

The learned detector interpretation adapts to the **specific traffic characteristics** of the simulation environment:

###### Traffic Demand Distribution

**Training Traffic Patterns:**

- Vehicle demand: 100-1000 veh/hr per intersection (uniform random per episode)
- Bicycle demand: 100-1000 bikes/hr per intersection
- Pedestrian demand: 100-1000 peds/hr per intersection
- Bus frequency: 4 buses per hour per direction (every 15 minutes)

The agent's detector interpretation reflects these distributions:

- Higher sensitivity to vehicle/bicycle detectors (high volume, high variability)
- Lower bus detector weight (low frequency, high priority when present)

###### Modal Interaction Patterns

The agent learns how detector co-activations correlate with reward:

- Vehicle + bicycle detectors both active simultaneously ‚Üí Both modes being served in current phase, assess if
  sufficient service provided
- Bus detector active + vehicle detectors clear ‚Üí Opportunity for efficient bus service without vehicle delay
- All detectors empty ‚Üí Rare event, agent learns to advance phases rapidly (minimize idle green)

###### Coordination Requirements

With 300m intersection spacing and centralized control, the agent discovers:

- Detector patterns indicating platoon arrivals from upstream intersection
- Timing relationships between downstream queue buildup and upstream phase changes
- Implicit green wave coordination emerges from cumulative reward maximization across both intersections

##### Detector Infrastructure as Perceptual Foundation

The detector network serves as the **perceptual foundation** upon which the DRL agent constructs its control policy:

**Physical Detector Configuration:**

- **Total detectors:** 32 induction loop detectors
    - 8 vehicle detectors per intersection √ó 2 intersections = 16 vehicle detectors
    - 8 bicycle detectors per intersection √ó 2 intersections = 16 bicycle detectors
- **Phase-specific activation:** Each phase has 4 vehicle + 4 bicycle detectors active
- **Detection logic:** Binary occupancy (triggered within last 3 seconds = 1.0, else = 0.0)
- **Coverage:** All major approaches monitored for both vehicle and bicycle traffic

**State Space Representation (per intersection):**

1. **Phase encoding:** 4 dimensions (one-hot encoding of current phase)
2. **Phase duration:** 1 dimension (normalized, capped at 60s)
3. **Vehicle queues:** 4 dimensions (current phase detectors)
4. **Bicycle queues:** 4 dimensions (current phase detectors)
5. **Bus presence:** 1 dimension (binary: bus detected on priority lanes)
6. **Bus waiting time:** 1 dimension (normalized)
7. **Simulation time:** 1 dimension (normalized 0-1)

**Total state dimension:** 16 per intersection √ó 2 intersections = 32 dimensions

**Neural network processing:**

1. **Nonlinear integration:** Captures complex detector pattern interactions across phases
2. **Temporal context:** Phase duration provides implicit memory of green time
3. **Adaptive processing:** Weights adjust to traffic characteristics during training
4. **Predictive capability:** Maps detector patterns to long-term cumulative reward

The Q-network functions as an **adaptive signal processing system** that translates raw detector activations (binary
occupancy states) into near-optimal control decisions, with the multi-objective reward function guiding the learning
process toward policies that balance efficiency, safety, equity, and sustainability.

##### Summary

Detector integration in the DRL architecture operates on three levels:

1. **Observational Level**: Detectors provide 20/32 (62.5%) of state features, capturing multimodal queue presence, bus
   priority status, and temporal context
2. **Learning Level**: Temporal difference learning discovers optimal detector-action mappings through 100+ training
   episodes, progressively refining interpretation from random to strategic
3. **Policy Level**: Learned Q-function implicitly encodes complex detector interpretation rules (adaptive thresholds,
   cross-modal balancing, predictive prioritization) that emerge from reward maximization rather than explicit
   programming

This detector-driven learning paradigm enables the DRL agent to achieve **adaptive, context-aware traffic control** that
responds to real-time sensor data while optimizing long-term multi-objective performance‚Äîa capability fundamentally
beyond rule-based actuated systems with fixed decision thresholds.

---

---
